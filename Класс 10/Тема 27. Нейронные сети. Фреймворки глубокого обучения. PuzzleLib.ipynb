{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"185.25px","left":"381.8px","right":"20px","top":"119px","width":"324.4px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Тема 27. Нейронные сети. Фреймворки глубокого обучения. PuzzleLib.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ksOIFy4GZ4Yf"},"source":["# Тема № 27. Нейронные сети, глубокое обучение. Фреймворки.\n"]},{"cell_type":"markdown","metadata":{"id":"9mo1o09vZ4Ym"},"source":["## Глубокое обучение\n","Модели, с которыми мы с вами успели познакомиться довольно простые, существуют гораздо более сложные модели, которые занимают много места, считаются долго, но зато дают хорошие результаты.\n","\n","На примере бустинга и стемминга мы видели, что модели можно соединять друг с другом, и тут возникла идея, а что если соединять между собой простые модельки, но сделать связь между ними более сложной, не даст ли это хорошего результата. \n","\n","Так появились нейронные сети, сети из простых соединенных между собой моделей (их называют нейронами), в которых соединения имеют настраиваемые веса - изменяя вес связи нейронов мы можем получать нужную сложность сети в целом. В зависимости от того, как именно соединены нейроны, получаем разные типы нейронных сетей. Нейроны могут располагаться в слоях, когда они работают одновременно (параллельно), или могут быть соединены последовательно, тогда работа следующего зависит от предыдущего, или вообще любым другим способом. Когда в нейронной сети много слоев нейронов, то такую модель называют \"глубокой\" (иногда \"глубинной\", англ. deep), а обучение таких глубоких моделей - **глубоким обучением** (deep learning). \n","\n","Как выглядят и работают глубокие нейронные сети мы узнаем в следующем классе, а сейчас получим лишь некоторые сведения, которые потребуются в дальнейшем."]},{"cell_type":"markdown","metadata":{"id":"pRjmbdclZ4Yq"},"source":["## Графические ускорители\n","Глубокие модели огромны, содержат  тысячи, сотни тысяч и миллионы параметров, которые нужно обучать. Это требует мощных компьютеров. Все настолько серьезно, что обычные компьютеры, которыми вы пользуетесь, слабо подходят для глубокого обучения.\n","\n","Как вообще сделать компьютер быстрее? Есть два варианта:\n","- повышать тактовую частоту процессора, которая показывает как часто происходят вычисления. Сегодня привычны процессоры с 3 гигагерцами тактовой частоты, что значит что за одну секунду может произойти 3 *миллиарда* переключений (вычислений). Долгое время компьютеры развивались именно повышением тактовой частоты, но всему есть предел. Теперь повышать тактовую частоту все сложней и сложней, и конечно дороже, это связано с физикой, из за того, что изменения тока в одном проводнике создают электромагнитные колебания, которые просачиваются в соседние проводники, и никуда от этого не денешься. Поэтому сейчас повысить, скажем, в два раза тактовую частоту будет стоить огромных денег, но ускорять компьютер необходимо.\n","- поэтому есть другой путь для ускорения - делать вычисления одновременно (параллельно) несколькими процессорами и как-то подгадать, чтобы наши алгоритмы могли работать параллельно. Тактовая частота не изменится, но за один такт, имея несколько процессоров, мы можем произвести больше вычислений. И именно по этому пути сегодня развивается вычислительная техника.\n","\n","Но на пути параллельных вычислений есть свои трудности. Если ваш алгоритм последователен - для следующего действия требуется результат предыдущего - то его никак не выполнить параллельно, один процессор должен ждать, пока не закончит другой. Нужны другие алгоритмы. Оказалось, что и нейронные сети и многие алгоритмы машинного обучения параллельны, значит хорошо подходят для использования на параллельной вычислительной технике.\n","\n","Сколько процессоров (микропроцессоров, central processing unit, CPU) может быть в обычном компьютере? Два? Четыре? Восемь? Не больше. Значит можно распараллелить вычисления всего-то в восемь раз. Сегодня этого мало. Нужны другие процессоры. Но вот удивительно, в обычном современном компьютере есть такое место, где этих процессоров может быть тысячи. Это **графические ускорители** (графическая плата, видеоплата, graphical processing unit, GPU). Оказывается те графические ускорители, которые вы используете для игрушек, на самом-то деле являются примером параллельных вычислителей, и могут выполнять сложные вычисления.\n","\n","Особенно отличилась компания NVIDIA которая теперь создает GPU не только для игрушек но и для серьезных расчетов. Именно на таких GPU обучаются большие модели. Почему же? В чем причина такой популярности GPU?\n","\n","Давайте посмотрим на картинку со схемой типичного CPU и GPU.\n","\n","![img](https://drive.google.com/uc?id=1Yv-8ed37V7LDeU3z9TGEOCNT_yeWIVhh)\n","\n","У CPU есть несколько (2-8) арифметико-логических устройств (ALU, зеленые) которые выполняют вычисления и могут работать параллельно, есть довольно большая и быстрая память кэша (cache) которая доступна всем ALU и которая используется для подгрузки нужных данных и память DRAM помедленнее, которую используют для хранения данных и сложное устройство управления (control), которое управляет всеми этими устройствами по отдельности. \n","\n","У GPU же есть много (сотни) ALU, но они гораздо более простые, работают на меньшей частоте, памяти DRAM существенно меньше (но возможно она быстрее), и устройство управления попроще, поскольку здесь нельзя управлять каждым ALU по отдельности, а только группой связанных ALU. Такая архитектура с одной стороны очень подходит для параллельных вычислений, но с другой сильно ограничивает возможности. К примеру, если в группе у вас 32 ALU, а нужно сложить только два числа, то все остальные все равно будут заниматься сложением чисел, хоть это вам и не надо, т.е. выполнять лишнюю бесполезную работу - это из-за того, что можно управлять группой целиком, а не отдельным ALU. **Не все можно эффективно распараллелить!**. \n","\n","Программирование параллельных вычислителей гораздо сложней, но к счастью, сегодня есть средства, которые делают работу за нас."]},{"cell_type":"markdown","metadata":{"id":"QlK9NvbXZ4Yt"},"source":["## Фреймворки глубокого обучения\n","Созданы **фреймворки глубокого обучения** специальные библиотеки программ, которые позволяют работать с нейронными сетями (и другими алгоритмами машинного обучения) и уже из коробки могут задействовать всю мощь GPU. Познакомимся кратко с некоторыми из них.\n","\n","\n","Основой любого фреймворка глубокого обучения является **автодифференцирование**. Мы уже упоминали, что самым популярным методом обучения является градиентный спуск и его модификации, для чего необходимо считать производные функции ошибки по всем настраиваемым параметрам. Вспомним школьное правило дифференцирования сложных (составных) функций: пусть, например, функция А зависит от функций В и С, которые зависят от аргумента х, тогда производная A по x получается как: \\\\(  A(B(x),C(x)) ==> \\frac{\\partial A}{\\partial x} = \\frac{\\partial A}{\\partial B} * \\frac{\\partial B}{\\partial x} + \\frac{\\partial A}{\\partial C} * \\frac{\\partial C}{\\partial x}\\\\)\n","\n","Значит для любой дифференцируемой функции мы можем найти производную, составив дерево зависимостей функций друг от друга и применяя такие правила. Даже если наши \"функции\" это программы, которые принимают некоторые аргументы, возвращают некоторые значения. Например программа А - складывает два аргумента, В - умножает входной аргумент на 3, С - возводит входной аргумент в квадрат, тогда получим:\n","\n","$$ A=B+C ==> \\frac{\\partial A}{\\partial B} = 1,   \\frac{\\partial A}{\\partial C} = 1, \\\\\n","B=3*x ==> \\frac{\\partial B}{\\partial x} = 3 \\\\ \n","C=x*x ==> \\frac{\\partial C}{\\partial x} = 2 * x \\\\ \n","и\\ окончательно\\ \\frac{\\partial A}{\\partial x} = 1 * 3+1 * (2 * x) $$\n","\n","Если мы знаем производные выхода программы по ее входам, то можем найти такие производные и для других программ, которые вызывают первую. И конечно это можно делать автоматически. При вызове программ нужно записывать дерево вычислений - какие программы друг друга вызывали, а когда потребуется вычислить производную - посмотреть на это дерево вычислений и применить правила для расчета производных составных функций. Если все действия в программе дифференцируемы (или могут быть принудительно сделаны дифференцируемыми), то можем найти и производную по любому аргументу этой программы. Это и есть автодифференцирование.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P6ZmlRDTZ4Yv"},"source":["В фреймворках модели представляют собой некоторые объекты, которые поддерживают автодифференцирование. И тогда создавая модель, например добавляя в нее слои, функции, входы и пр. мы можем потом искать и производные и тем самым обучать модель.\n","\n","Один из самых известных фреймворков глубокого обучения [TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) и его надстройка [Keras](https://keras.io/getting_started/). Вот как выглядит пример реализации модели на нем:\n","\n","``\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10)\n","])\n","``\n","\n","Создается последовательная модель  `Sequential` в которую постепенно добавляются слои нейронов или функций, выход одного слоя является входом  следующего, а это и есть дерево связей, здесь оно последовательное, без разветвлений. Конечно для моделей реализованы методы для обучения разными способами, расчета выходов и др., они могут называться по-другому чем в `sklearn` но смысл их такой же. Например метод `.fit()` что делает? Правильно, обучает модель на заданных примерах, а метод `.evaluate()`? Можно догадаться по названию (а лучше конечно смотреть в документации), что это оценка работы метода для заданных примеров (в `sklearn` в точности такого же метода нет, но похоже на `.score()`).\n","\n","\n","Другой известный фреймворк [PyTorch](https://pytorch.org/tutorials/) тоже строит граф вычислений и позволяет посмотреть на него (функция `make_dot` из библиотеки `torchviz`). \n","\n","Посмотрим на пример:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMr28HTAbhqf","executionInfo":{"status":"ok","timestamp":1613321103433,"user_tz":-180,"elapsed":5569,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"0501e940-8fe2-4a76-d2c7-59a39832397a"},"source":["!pip install torchviz"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torchviz\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n","\r\u001b[K     |████████                        | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.7.0+cu101)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.19.5)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3522 sha256=07a8692f3dc3bc4f207cd187d225ad0455c5972b4538c13443225f8449d22893\n","  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-11-05T09:38:39.871947Z","start_time":"2020-11-05T09:38:39.746939Z"},"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"xiUSYNpPZ4Yx","executionInfo":{"status":"ok","timestamp":1613321114455,"user_tz":-180,"elapsed":671,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"7fe316cb-6182-4397-d7cf-d55df5107f48"},"source":["import torch # подключаем torch\n","\n","# задаем вычисления\n","x = torch.tensor(1.0, requires_grad = True) # исходная переменная х\n","y = torch.tensor(2.0,requires_grad = True) # исходная переменная у\n","\n","#какие-то вычисления:\n","z = x * y # перемножаем\n","c=x*x # квадрат\n","d=y-z # вычитание\n","e=c**d # степень\n","from torchviz import make_dot # для просмотра графа вычислений\n","params=dict(x=x,y=y) # названия переменных\n","make_dot(e,params=params)  # строим граф"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<graphviz.dot.Digraph at 0x7fc7de16c3c8>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"227pt\" height=\"213pt\"\n viewBox=\"0.00 0.00 226.50 213.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 209)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-209 222.5,-209 222.5,4 -4,4\"/>\n<!-- 140496400680328 -->\n<g id=\"node1\" class=\"node\">\n<title>140496400680328</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"146,-21 53,-21 53,0 146,0 146,-21\"/>\n<text text-anchor=\"middle\" x=\"99.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward1</text>\n</g>\n<!-- 140496400679936 -->\n<g id=\"node2\" class=\"node\">\n<title>140496400679936</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"91,-135 0,-135 0,-114 91,-114 91,-135\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140496400679936&#45;&gt;140496400680328 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140496400679936&#45;&gt;140496400680328</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.4834,-113.9795C59.479,-94.9888 78.5634,-54.6995 90.0855,-30.3751\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.2533,-31.8633 94.3711,-21.3276 86.9271,-28.8667 93.2533,-31.8633\"/>\n</g>\n<!-- 140498232258800 -->\n<g id=\"node3\" class=\"node\">\n<title>140498232258800</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"72.5,-205 18.5,-205 18.5,-171 72.5,-171 72.5,-205\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">x</text>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140498232258800&#45;&gt;140496400679936 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140498232258800&#45;&gt;140496400679936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M39.4426,-170.9832C38.6465,-163.1157 38.5367,-153.6973 39.1131,-145.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"42.6015,-145.7029 40.2662,-135.3687 35.6473,-144.9037 42.6015,-145.7029\"/>\n</g>\n<!-- 140498232258800&#45;&gt;140496400679936 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140498232258800&#45;&gt;140496400679936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.5574,-170.9832C52.3535,-163.1157 52.4633,-153.6973 51.8869,-145.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.3527,-144.9037 50.7338,-135.3687 48.3985,-145.7029 55.3527,-144.9037\"/>\n</g>\n<!-- 140498232323600 -->\n<g id=\"node6\" class=\"node\">\n<title>140498232323600</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"200,-135 109,-135 109,-114 200,-114 200,-135\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140498232258800&#45;&gt;140498232323600 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140498232258800&#45;&gt;140498232323600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72.7241,-172.1401C89.4302,-162.4076 110.7433,-149.9913 127.4224,-140.2746\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"129.2743,-143.2464 136.1531,-135.1883 125.7506,-137.1979 129.2743,-143.2464\"/>\n</g>\n<!-- 140496400679488 -->\n<g id=\"node4\" class=\"node\">\n<title>140496400679488</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"199.5,-78 109.5,-78 109.5,-57 199.5,-57 199.5,-78\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140496400679488&#45;&gt;140496400680328 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140496400679488&#45;&gt;140496400680328</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M144.1549,-56.7787C136.4728,-48.8173 125.8169,-37.7739 116.8411,-28.4717\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"119.2611,-25.9391 109.7987,-21.1732 114.2237,-30.7997 119.2611,-25.9391\"/>\n</g>\n<!-- 140498231882304 -->\n<g id=\"node5\" class=\"node\">\n<title>140498231882304</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"218.5,-205 164.5,-205 164.5,-171 218.5,-171 218.5,-205\"/>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">y</text>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140498231882304&#45;&gt;140496400679488 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140498231882304&#45;&gt;140496400679488</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M201.8775,-170.9148C209.8161,-155.4971 218.0709,-132.4699 209.5,-114 203.7687,-101.6494 192.9622,-91.3925 182.4349,-83.6827\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.3922,-80.7812 174.1543,-78.0544 180.4572,-86.5705 184.3922,-80.7812\"/>\n</g>\n<!-- 140498231882304&#45;&gt;140498232323600 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140498231882304&#45;&gt;140498232323600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M181.5847,-170.9832C176.7429,-162.6737 170.8931,-152.6342 165.8709,-144.015\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.8915,-142.2469 160.8329,-135.3687 162.8434,-145.771 168.8915,-142.2469\"/>\n</g>\n<!-- 140498232323600&#45;&gt;140496400679488 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140498232323600&#45;&gt;140496400679488</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M154.5,-113.7787C154.5,-106.6134 154.5,-96.9517 154.5,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.0001,-88.1732 154.5,-78.1732 151.0001,-88.1732 158.0001,-88.1732\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"VBUgtLCWZ4Y1"},"source":["Отображаются исходные переменные и действия, которые с ними проводились. По названию можно догадаться что за действия: Mul - умножение, Sub - вычитание, Pow - степень.\n","\n","Посмотрите на картинку и объясните каждую стрелку, найдите и укажите место где получаются переменные z, c, d, e.\n","\n","Сделайте аналогично для своих примеров вычислений."]},{"cell_type":"markdown","metadata":{"id":"jKTHlpVVZ4Y2"},"source":["## Библиотека PuzzleLib\n","Вообще всяких разных фреймворков глубокого обучения сделано уже много, см., например, [здесь](https://towardsdatascience.com/top-10-best-deep-learning-frameworks-in-2019-5ccb90ea6de). \n","\n","Россия не отстает, и наши компании тоже делают похожие фреймворки, с одним из них мы познакомимся поближе и будем использовать его в дальнейшем, это библиотека [PuzzleLib](https://puzzlelib.org/). Она создана совсем недавно, и разнообразие функций не так велико, как хотелось бы, но библиотека развивается, и на некоторых тестах даже превосходит известные фреймворки. Что особенно приятно - библиотека имеет документацию на русском языке. Разработчики библиотеки очень серьезно относятся к удобству именно русскоязычных пользователей, собственно данный курс и создан с этой целью.   "]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-07-28T10:45:28.440786Z","start_time":"2020-07-28T10:45:28.422785Z"},"id":"A3GDXXS0Z4Y3"},"source":["Библитотека состоит из нескольких разделов, содержащих необходимые инстурменты для создания и обучения своей нейронной сети.\n","\n","- [Modules](https://puzzlelib.org/documentation/base/modules/Modules/): Основные блоки искусственных нейронных сетей, разные типы слоев нейронов, функции преобразования.\n","- [Containers](https://puzzlelib.org/documentation/base/containers/Containers/): инструменты для соединения модулей,  установки связей между слоями нейронной сети. Позволяет соединять слои параллельно, последовательно и в виде заданного графа.\n","- [Cost](https://puzzlelib.org/documentation/base/cost/Costs/): различные функции ошибки.\n","- [Optimizers](https://puzzlelib.org/documentation/base/optimizers/Optimizers/): методы оптимизации функции ошибки, основанные на градиентном спуске.\n","- [Handlers](https://puzzlelib.org/documentation/base/handlers/Handlers/): вспомогательные инструменты для облегчения обучения, расчета и проверки нейронных сетей.\n","- [Transformers](https://puzzlelib.org/documentation/base/transformers/Transformers/): инструменты для преобразования обучающих данных налету, в процессе обучения.\n"]},{"cell_type":"markdown","metadata":{"id":"lDJPYbHLZ4Y4"},"source":["Подробно про эту библиотеку мы поговорим на следующих уроках, когда будем изучать нейронные сети. "]},{"cell_type":"code","metadata":{"id":"5_YWmoFXZ4Y6"},"source":[""],"execution_count":null,"outputs":[]}]}