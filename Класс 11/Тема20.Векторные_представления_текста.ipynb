{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Тема20.Векторные_представления_текста.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"PuM9XvCC23uF"},"source":["# Тема 20. Векторные представления слов."]},{"cell_type":"markdown","metadata":{"id":"eLm3meyF23uR"},"source":["## Введение\n","\n","Вспомним Тему 8 о векторных представлениях изображений. Мы увидели, что изображение можно описать массивом чисел - выходом какого-то из слоев нейронной сети - настолько точно, что по этому массиву можно провести классификацию изображения, отличить одно от другого.\n","\n","Раз можно так делать для изображений, то можно и для других типов данных, для звуков, сигналов и, конечно, для текстов.\n","\n","Раньше мы использовали векторные представления внутри нейронной сети, чтобы проводить классификацию, но оказывается, векторные представления интересны и сами по себе. \n","\n","Давайте введем простые векторные представления для текстов и посмотрим, какие у них получаются интересные свойства.\n"]},{"cell_type":"markdown","metadata":{"id":"psYi8z3h23uT"},"source":["# Унитарное кодирование\n","Пожалуй самым простым векторным представлением является *унитарное кодирование* (по-английски one-hot encoding).\n","\n","Возьмем какой-либо текст, разобьем его на слова-элементы и составим словарь таких слов, в котором каждому слову будет приписан его номер в словаре. \n","\n","Этот номер слова можно представить в бинарном виде, сопоставим каждому слову в словаре *бинарный вектор*, состоящий из нулей и только одной единицы. Положение единицы в векторе показывает номер слова.\n","\n","![img](https://drive.google.com/uc?id=1hDY2EKB7ND-Uxq9FnKVtxC-U-0bnuiTe)\n"," \n","В векторе будет столько элементов, сколько слов в словаре мы записали.\n","\n","Обратите внимание, что единичка, по-сути, является признаком слова: стоит единичка в пятом элементе, значит слово номер пять.\n","\n","Вообще, в словарь мы можем добавлять какие угодно \"слова\", это могут быть обычные привычные нам слова, части слов и даже отдельные буквы, словосочетания и целые предложения, знаки препинания, смайлики, и прочее. Главное, что у всех этих \"слов\" есть номер, представленный унитарным вектором.\n","\n","Текст является последовательностью слов, а в унитарном представлении - последовательностью унитарных векторов.\n","\n","![img](https://drive.google.com/uc?id=1uEodrFkeG433O_zCWf3EpR8JVPYEFysx)\n","\n","Унитарное кодирование используется и для других типов информации, для изображений, сигналов и пр. Хорошим свойством является то, что унитарное представление не зависит от длины слова, ведь в словарь мы можем добавлять слова разной длины, хоть из одной буквы, хоть целое словосочетание.\n","\n","Однако такое представление не дает нам никаких новых интересных свойств, ведь унитарные вектора никак не связаны между собой и зависят от нашего произвола - в каком порядке слова в словарь добавляем, такой вектор и получим. \n","\n","Поэтому создают другие, более интересные, векторные представления."]},{"cell_type":"markdown","metadata":{"id":"x_7KaRdJ23uT"},"source":["# Word2vec\n","Хочется как-то связать между собой слова и их векторные представления, ведь в реальных текстах слова, конечно же, связаны между собой **смыслом**. Вряд ли кто-то захочет читать тексты из случайных слов.\n","\n","Если есть взаимосвязь между словами текста, то ее можно уловить, вычислить.\n","\n","А давайте попробуем сделать так: \n","- возьмем из текста три слова подряд.\n","- закроем одно слово и попробуем его угадать по двум оставшимся.\n","\n","Во многих случаях, но конечно не всегда, это нам удастся. Попробуйте угадать какое слово скрыто под звездочками во фразе \"Мама мыла ....\".\n","Если вы сказали \"раму\", то угадали, но могли сказать \"пол\" и не угадали. \n","\n","Давайте возьмем больше слов \"Мама мыла оконную ...\": тут легче угадать слово \"раму\", но все равно есть и другие варианты (придумайте).\n","\n","И все же, для многих последовательностей слов можно угадать какое слово закрыто, так пусть этим займется компьютер.\n","\n","Текст это последовательность слов, слова можно представить унитарными векторами. Нам надо лишь по нескольким унитарным векторам вычислять следующий. Это абсолютно тоже самое, как если бы мы хотели обучить нейронную сеть. Известны входы - несколько унитарных векторов, знаем выход - один унитарный вектор, давайте обучим.\n","\n","Можно посмотреть и с другой стороны, если по нескольким словам мы угадываем следующее, то давайте наоборот, по слову угадаем какие слова ему предшествовали. Технически задача такая же. Знаем вход - один унитарный вектор, знаем выход - несколько унитарных векторов - так давайте обучим нейронную сеть.\n","\n","Эти подходы можно совместить, по нескольким векторам угадываем один пропущенный, а затем по нему угадываем обратно какие вектора были изначально, или, другими словами, угадываем слово по словам его окружающим, а затем по этому слову угадываем окружающие слова. Обычно угадывают среднее слово из трех, но это вовсе не обязательно.\n","\n","Такая технология получила название **word2vec** (произносится \"ворд ту век\"). На картинке показана для пяти слов, обозначенных буквой V с индексом в скобках.  Первую половинку - угадывание слова по окружению - назвали CBOW, вторую - угадывания окружения по слову - SkipGram. \n","\n","![img](https://drive.google.com/uc?id=18pa20uv6xi8tGC6pdOS3Vc11KUUcpW2e)"]},{"cell_type":"markdown","metadata":{"id":"yeHc1Oxn23uU"},"source":["Посмотрим на CBOW (это сокращение от Continuous Bag of Word, непрерывный мешок слов).\n","\n","Мы можем использовать простую нейронную сеть только с одним скрытым слоем для него.\n","\n","Унитарные вектора слов-окружения переводятся в скрытом слое в некоторые выходы, а затем по выходам этого скрытого слоя считается унитарный вектор исходного слова.\n","\n","Перевод унитарного представления слов-окружения в скрытом слое производится, как мы понимаем из работы нейронной сети, умножением на матрицу весов W. Ее размер (число нейронов в скрытом слое) * (число слов в словаре). Нет необходимости делать эту матрицу разной для разных слов-окружений, пусть будет одна.\n","\n","Аналогично, переход из скрытого слоя на выход также делается с помощью умножения на матрицу W'. Ее размер (число слов в словаре) * (число нейронов в скрытом слое). \n","\n","Матрица W' нам не важна, но посмотрим более внимательно на матрицу W.\n","\n","Она умножается на унитарный вектор, т.е. вектор, в котором все элементы нули кроме одного. Что произойдет при таком умножении? \n","\n","Правильно, выберется только один столбец из матрицы, с таким номером, на какой позиции стоит единичка во входом векторе. Но положение этой единички означает номер слова в словаре! Значит, каждому слову из словаря соответствует свой вектор-столбец в матрице W. Его размерность определяется числом нейронов в скрытом слое, которое мы задаем сами при обучении. **Эти вектора-столбцы и являются новым векторным представлением слов**.   \n","\n","Итак, вкратце, еще раз.\n","- Берем тексты\n","- разбиваем их на последовательность слов\n","- переводим слова в унитарные вектора\n","- на парах (соседние слова)-(среднее слово) обучаем простую нейронную сеть CBOW с ее матрицей W.\n","- используем столбцы матрицы W как новое векторное представление слов. Длина этих векторов задается произвольно при обучении.\n","\n","![img](https://drive.google.com/uc?id=1bTpfqH0niwJKwMRHKdNbuOFiR9jucCZe)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hqo4y3v523uV"},"source":["# Геометрия слов\n","Создав векторные представления для слов, можно их использовать в различных *геометрических* операциях: сложить вектора, вычесть, найти угол меду ними.\n","\n","И, что наиболее удивительно в таких векторных представлениях, эти геометрические операции с векторами имеют **смысл** с точки зрения самих слов.\n","\n","Например:\n","* Король - мужчина + женщина = Королева\n","* Великобритания - Лондон + Москва = Россия\n"," и др.\n"," \n"," Прежде чем смотреть на примеры, несколько слов о сравнении векторов.\n"," \n","## Косинусное расстояние.\n","\n","Вектора можно сравнивать между собой, логично, что близкие вектора означают близкие по смыслу слова. Но как именно сравнивать вектора?\n","\n","Оказалось, что постое Евклидово расстояние между векторами не так интересно, более интересно сравнивать угол между векторами или, правильнее, косинус угла. \n","\n","Ну-ка, вспоминайте геометрию, как посчитать косинус угла между векторами?\n","\n","А вот так: скалярное произведение векторов поделить на длины этих векторов. \n","\n","![img](https://drive.google.com/uc?id=1vxVWdSAjY5oK6U7CtGadB0-2T5YLFmSh)\n","\n","Часто векторные представления *нормализуют*, т.е. приводят к единичной длине, тогда ее и считать не надо.\n","\n","Итак, мера схожести векторов - косинус угла между ними. Близкие по смыслу слова скорей всего дадут близкие по углу вектора."]},{"cell_type":"markdown","metadata":{"id":"_BOipolf23uW"},"source":["# Библиотека gensim\n","\n","А теперь примеры. Поиграть с векторами онлайн вы можете [здесь](https://rare-technologies.com/word2vec-tutorial/#app), но давайте и сами реализуем.  \n","\n","Нам поможет библиотека [gensim](https://radimrehurek.com/gensim/index.html). Установим ее.\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:16.961904Z","start_time":"2021-01-30T17:11:14.272750Z"},"id":"IXyn2JWv23uW","outputId":"272daae1-4d48-4b91-9aac-f35faf68cc1a"},"source":["#!pip install gensim"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gensim in c:\\users\\neuron\\anaconda3\\lib\\site-packages (3.8.3)\n","Requirement already satisfied: Cython==0.29.14 in c:\\users\\neuron\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n","Requirement already satisfied: six>=1.5.0 in c:\\users\\neuron\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in c:\\users\\neuron\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n","Requirement already satisfied: numpy>=1.11.3 in c:\\users\\neuron\\anaconda3\\lib\\site-packages (from gensim) (1.19.1)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\neuron\\anaconda3\\lib\\site-packages (from gensim) (4.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0J6XlEf23uY"},"source":["Подключим вспомогательную библиотеку для записи действий."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:16.968904Z","start_time":"2021-01-30T17:11:16.965904Z"},"id":"PtqCCWh423uY","executionInfo":{"status":"ok","timestamp":1625394587432,"user_tz":-180,"elapsed":12,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2HHRtF923uZ"},"source":["Скачаем уже обученную модель word2vec, она была обучена на огромном наборе текстов на английском языке из 3 млн. слов и занимает около 2 Гигабайт, придется подождать. Это надо сделать только один раз. Модель загружается в виде специального объекта, в котором прописаны многие методы для работы с векторами.\n","\n","*Для учителя: иногда сайт обрывает связь и закачку, рекомендуется заранее скачать массивы использую методы load(), save()*."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:57.030196Z","start_time":"2021-01-30T17:11:16.969904Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"zw26YKfY23ua","executionInfo":{"status":"ok","timestamp":1625395607526,"user_tz":-180,"elapsed":607602,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"0874f851-4147-4e0f-db31-86530d124fc9"},"source":["import gensim.downloader as api # \n","wv = api.load('word2vec-google-news-300') # "],"execution_count":4,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["2021-07-04 10:44:21,974 : INFO : word2vec-google-news-300 downloaded\n","2021-07-04 10:44:21,978 : INFO : loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n","2021-07-04 10:46:47,132 : INFO : loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"prcvk_j023ua"},"source":["Например, выведем на экран первые 100 слов из словаря, которые хранятся в итерируемом поле .index2word"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:57.045196Z","start_time":"2021-01-30T17:11:57.031196Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"QeBsRaP_23ub","executionInfo":{"status":"ok","timestamp":1625395620639,"user_tz":-180,"elapsed":270,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"8481a2df-3111-472e-c0ce-005643dce002"},"source":["for index, word in enumerate(wv.index2word):\n","    if index == 100:\n","        break\n","    print(f\"word #{index}/{len(wv.index2word)} is {word}\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["word #0/3000000 is </s>\n","word #1/3000000 is in\n","word #2/3000000 is for\n","word #3/3000000 is that\n","word #4/3000000 is is\n","word #5/3000000 is on\n","word #6/3000000 is ##\n","word #7/3000000 is The\n","word #8/3000000 is with\n","word #9/3000000 is said\n","word #10/3000000 is was\n","word #11/3000000 is the\n","word #12/3000000 is at\n","word #13/3000000 is not\n","word #14/3000000 is as\n","word #15/3000000 is it\n","word #16/3000000 is be\n","word #17/3000000 is from\n","word #18/3000000 is by\n","word #19/3000000 is are\n","word #20/3000000 is I\n","word #21/3000000 is have\n","word #22/3000000 is he\n","word #23/3000000 is will\n","word #24/3000000 is has\n","word #25/3000000 is ####\n","word #26/3000000 is his\n","word #27/3000000 is an\n","word #28/3000000 is this\n","word #29/3000000 is or\n","word #30/3000000 is their\n","word #31/3000000 is who\n","word #32/3000000 is they\n","word #33/3000000 is but\n","word #34/3000000 is $\n","word #35/3000000 is had\n","word #36/3000000 is year\n","word #37/3000000 is were\n","word #38/3000000 is we\n","word #39/3000000 is more\n","word #40/3000000 is ###\n","word #41/3000000 is up\n","word #42/3000000 is been\n","word #43/3000000 is you\n","word #44/3000000 is its\n","word #45/3000000 is one\n","word #46/3000000 is about\n","word #47/3000000 is would\n","word #48/3000000 is which\n","word #49/3000000 is out\n","word #50/3000000 is can\n","word #51/3000000 is It\n","word #52/3000000 is all\n","word #53/3000000 is also\n","word #54/3000000 is two\n","word #55/3000000 is after\n","word #56/3000000 is first\n","word #57/3000000 is He\n","word #58/3000000 is do\n","word #59/3000000 is time\n","word #60/3000000 is than\n","word #61/3000000 is when\n","word #62/3000000 is We\n","word #63/3000000 is over\n","word #64/3000000 is last\n","word #65/3000000 is new\n","word #66/3000000 is other\n","word #67/3000000 is her\n","word #68/3000000 is people\n","word #69/3000000 is into\n","word #70/3000000 is In\n","word #71/3000000 is our\n","word #72/3000000 is there\n","word #73/3000000 is A\n","word #74/3000000 is she\n","word #75/3000000 is could\n","word #76/3000000 is just\n","word #77/3000000 is years\n","word #78/3000000 is some\n","word #79/3000000 is U.S.\n","word #80/3000000 is three\n","word #81/3000000 is million\n","word #82/3000000 is them\n","word #83/3000000 is what\n","word #84/3000000 is But\n","word #85/3000000 is so\n","word #86/3000000 is no\n","word #87/3000000 is like\n","word #88/3000000 is if\n","word #89/3000000 is only\n","word #90/3000000 is percent\n","word #91/3000000 is get\n","word #92/3000000 is did\n","word #93/3000000 is him\n","word #94/3000000 is game\n","word #95/3000000 is back\n","word #96/3000000 is because\n","word #97/3000000 is now\n","word #98/3000000 is #.#\n","word #99/3000000 is before\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oqjsNfe223ub"},"source":["Получить вектор слова можно используя это слово как индекс. Получим для слова 'king'. Это вектор из 300 элементов. Конечно, это слово должно быть в словаре и именно в таком виде, иначе возникнет ошибка. Важен и регистр букв, попробуйте слово 'kinG'."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:57.087199Z","start_time":"2021-01-30T17:11:57.048197Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7rmj06m23uc","executionInfo":{"status":"ok","timestamp":1625395628759,"user_tz":-180,"elapsed":247,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"ca32717f-285b-466b-c6dd-41dd14d32f30"},"source":["vec_king = wv['king']\n","vec_king.shape\n","#vec_king = wv['kinG']"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300,)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Uzk2QWmI23uc"},"source":["Посчитать \"похожесть\" слов, т.е. их векторов, можно с помощью метода .similarity(). Чем больше число, тем более похожи слова."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:11:57.121201Z","start_time":"2021-01-30T17:11:57.089199Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"YvdK5kQR23uc","executionInfo":{"status":"ok","timestamp":1625395638096,"user_tz":-180,"elapsed":294,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"614fcf43-472e-48ec-c869-82adf10ed92c"},"source":["pairs = [                 # пары слов\n","    ('car', 'minivan'),   # минивэн это тип автомобиля\n","    ('car', 'bicycle'),   # мотоцикл имеет колеса как и автомобиль\n","    ('car', 'airplane'),  # ладно, самолет не колесное средство, но все же средство передвижения\n","    ('car', 'cereal'),    # зерно и автомобиль, хм..\n","    ('car', 'communism'), # какая связь между автомобилем и коммунизмом???\n","]\n","for w1, w2 in pairs:\n","    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["'car'\t'minivan'\t0.69\n","'car'\t'bicycle'\t0.54\n","'car'\t'airplane'\t0.42\n","'car'\t'cereal'\t0.14\n","'car'\t'communism'\t0.06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"huAtz1iv23ud"},"source":["Перебирать вручную все пары слов, чтобы найти самые похожие будет трудно, пусть компьютер сравнит все-все вектора и выведет самые похожие на заданный. \n","\n","Метод .most_similar() принимает набор слов для которых искать похожие (аргумент positive) и число похожих слов (аргумент topn)\n","\n","(Первый запуск может быть долгим, так как надо посчитать длины всех векторов, потом будет гораздо быстрее)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:12:01.986479Z","start_time":"2021-01-30T17:11:57.123201Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"29vJFCWf23ud","executionInfo":{"status":"ok","timestamp":1625395681312,"user_tz":-180,"elapsed":12085,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"81a51f04-cbd7-4ec4-bba0-a0e0abb14d0b"},"source":["print(wv.most_similar(positive=['man', 'woman'], topn=5))\n","print(wv.most_similar(positive=['Russia'], topn=5))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["2021-07-04 10:47:49,140 : INFO : precomputing L2-norms of word weight vectors\n"],"name":"stderr"},{"output_type":"stream","text":["[('teenage_girl', 0.7174351811408997), ('girl', 0.7137972712516785), ('teenager', 0.6865389347076416), ('boy', 0.6810464859008789), ('teen_ager', 0.5822051763534546)]\n","[('Ukraine', 0.7918288111686707), ('Moscow', 0.7575765252113342), ('Russian', 0.7464962005615234), ('Belarus', 0.7303562164306641), ('Kremlin', 0.7048990726470947)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Av4fcfol23ue"},"source":["Найти слово, которое непохоже на остальные? Легко, метод .doesnt_match(). Все слова (их вектора) будут сравнены между собой и выведется то, которое похоже меньше всех."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:12:01.995480Z","start_time":"2021-01-30T17:12:01.989479Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"vYnqSCA823ue","executionInfo":{"status":"ok","timestamp":1625395702759,"user_tz":-180,"elapsed":270,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"8d74df71-990c-461b-b556-efd1c05c729f"},"source":["print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["car\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FwgLBaTM23ue"},"source":["Геометрические операции, вектор 'France' минус вектор 'Paris' плюс вектор 'Moscow'. Получится какой-то вектор. Его может не быть в словаре, найдем ближайшие из словаря к нему, метод .similar_by_vector() "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:12:02.254494Z","start_time":"2021-01-30T17:12:01.997480Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"NCYoxEHB23ue","executionInfo":{"status":"ok","timestamp":1625395759176,"user_tz":-180,"elapsed":1053,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"bca91959-989c-4897-da45-d573f048cbaf"},"source":["Russia=wv['France']-wv['Paris']+wv['Moscow']\n","wv.similar_by_vector(Russia)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Russia', 0.8497266173362732),\n"," ('Moscow', 0.7469395995140076),\n"," ('Ukraine', 0.7185336351394653),\n"," ('Belarus', 0.6865631341934204),\n"," ('Russian', 0.6784680485725403),\n"," ('Kremlin', 0.6503680944442749),\n"," ('Moldova', 0.6195235848426819),\n"," ('ex_Soviet', 0.6118861436843872),\n"," ('Kazakhstan', 0.607187032699585),\n"," ('Russians', 0.6063269376754761)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"4QS6pGzd23ue"},"source":["А на русском языке??\n","\n","Такие модели тоже есть, но они гораздо слабее, мало слов, мало текстов.\n","Загрузим модель word2vec-ruscorpora-300 . Всего-то 200 тысяч слов."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:12:05.004652Z","start_time":"2021-01-30T17:12:02.255494Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"pOWgBWZa23ug","executionInfo":{"status":"ok","timestamp":1625395850177,"user_tz":-180,"elapsed":77782,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"44f35d01-56df-4ca1-96df-1930799b0033"},"source":["wv_rus = api.load('word2vec-ruscorpora-300')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 198.8/198.8MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["2021-07-04 10:50:37,219 : INFO : word2vec-ruscorpora-300 downloaded\n","2021-07-04 10:50:37,227 : INFO : loading projection weights from /root/gensim-data/word2vec-ruscorpora-300/word2vec-ruscorpora-300.gz\n","2021-07-04 10:50:49,811 : INFO : loaded (184973, 300) matrix from /root/gensim-data/word2vec-ruscorpora-300/word2vec-ruscorpora-300.gz\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"XTcErjx923ug"},"source":["Здесь слова имеют приставки, показывающие их часть речи."]},{"cell_type":"markdown","metadata":{"id":"175L8SiI23ug"},"source":["что получится для король - мужчина + женщина ?\n","По идее королева. Но получится король. Но и \"правильный\" ответ тоже недалеко, второй по счету.   "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-30T17:12:05.304669Z","start_time":"2021-01-30T17:12:05.005652Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBtwTl2n23ug","executionInfo":{"status":"ok","timestamp":1625395855091,"user_tz":-180,"elapsed":657,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"0d4ccaa1-f510-4609-b0a6-f46c1d46c6cf"},"source":["queen= wv_rus['король_NOUN'] - wv_rus['мужчина_NOUN'] + wv_rus['женщина_NOUN']\n","\n","wv_rus.similar_by_vector(queen)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2021-07-04 10:50:54,343 : INFO : precomputing L2-norms of word weight vectors\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('король_NOUN', 0.880538821220398),\n"," ('королева_NOUN', 0.7313904166221619),\n"," ('герцог_NOUN', 0.6502388715744019),\n"," ('принцесса_NOUN', 0.6266285181045532),\n"," ('герцогиня_NOUN', 0.6240381002426147),\n"," ('королевство_NOUN', 0.6094207167625427),\n"," ('зюдерманландский_ADJ', 0.6084389686584473),\n"," ('дурлахский_ADJ', 0.6081665754318237),\n"," ('ульрик::элеонора_NOUN', 0.6073107719421387),\n"," ('максимилианов_NOUN', 0.6057003736495972)]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"EuD6ipd223uh"},"source":["# Задания\n","Успешность геометрических операций с векторами слов сильно зависит и от вида модели и от текстов, на которых она обучалась. Не все понятные человеку аналогии слов, получаются и в компьютере. В качестве упражнения попробуйте найти примеры логичных и нелогичных аналогий. "]},{"cell_type":"markdown","metadata":{"id":"NvOW0HC_JYAW"},"source":["# Векторные представления GloVe\n","\n","Word2Vec делает локальные связи между словами, теми, которые располагаются в предложении близко друг к другу. Но вполне может быть так, что и далекие слова тоже связаны между собой смыслом.\n","\n","Хотелось бы и такие случаи учесть. Придуман другой способ построения векторов слов - GloVe (Global Vectors). \n","\n","Идея относительно проста:\n","\n","Давайте посчитаем матрицу взаимной встречаемости $X_{ij}$ всех слов друг с другом, которая показывает как часто одно слово находится рядом (является контекстом) с другим. Это большая матрица размером по числу слов в словаре (миллионы). Давайте факторизуем эту матрицу, представим ее меньшим числом параметров, которые будем подбирать, чтобы получить как можно более точное представление такой матрицы.\n","\n","В GloVe каждому слову сопоставляется два вектора $w, w'$ и смещений $b, b'$ (если матрица встречаемости симметрична, они практически совпадают). Создают функцию ошибки \n","\n","$ J=\\sum_{i,j=1}^V f(X_{ij})(w_i^T*w'_j+b_i+b'_j - log(X_{ij}))^2 $\n","\n","которую минимизируют с целью такого подбора векторов (и смещений), чтобы их скалярное произведение (плюс смещения) давали логарифм матрицы встречаемости как можно точнее для всех комбинаций пар слов. Дополнительный множитель $f(X)$ изменяет влияние слишком частых и слишком редких пар слов.\n","\n","![img](https://www.researchgate.net/profile/Jeffrey-Pennington-2/publication/284576917/figure/fig1/AS:640924933697536@1529819820026/Weighting-function-f-with-a-3-4.png)\n","\n","Посмотрим пример векторов на текстах с Wikipedia.\n","\n"]},{"cell_type":"code","metadata":{"id":"6R9SNuFkKUIf","executionInfo":{"status":"ok","timestamp":1625396171438,"user_tz":-180,"elapsed":312,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["import numpy as np\n","\n","import os\n","import pickle\n","import time\n","\n","SENTENCE_LENGTH_MAX = 32\n","EMBEDDING_DIM=50"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdhY2mpPKXJr"},"source":["Используем библиотеку [glove-python](https://github.com/maciejkula/glove-python) для загрузки и работы с уже обученными векторными представлениями GloVe."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joG8Yt95KaWK","executionInfo":{"status":"ok","timestamp":1625396202419,"user_tz":-180,"elapsed":6088,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"d3e9518d-9305-422b-8abc-aeacdf242463"},"source":["! pip install glove-python-binary"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Collecting glove-python-binary\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/11/d8510a80110f736822856db566341dd2e1e7c3af536f77e409a6c09e0c22/glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948kB)\n","\r\u001b[K     |▍                               | 10kB 19.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 23.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 29.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 31.2MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 32.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 29.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 29.9MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 28.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102kB 29.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 204kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 235kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952kB 29.5MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n","Installing collected packages: glove-python-binary\n","Successfully installed glove-python-binary-0.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jZYNzN8VKeoC"},"source":["Загружаем обученные вектора (полученые из текста с 6 миллирдами токенами 50-мерные вектора): http://nlp.stanford.edu/projects/glove/ \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pgpJXLlKdx6","executionInfo":{"status":"ok","timestamp":1625396238885,"user_tz":-180,"elapsed":2380,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"186dcb2e-0fa7-4fcd-cd51-b36c57d573c1"},"source":["import glove\n","import os, requests, shutil\n","\n","glove_dir = './data/RNN/' # директория загрузки\n","glove_100k_50d = 'glove.first-100k.6B.50d.txt' # название файла с векторами\n","glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d) # путь к нему\n","\n","# Временные файлы на случай если не будут загружаться основные (медленно)\n","data_cache = './data/cache'\n","glove_full_tar = 'glove.6B.zip'\n","glove_full_50d = 'glove.6B.50d.txt'\n","\n","# Адрес загрузки\n","download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n","original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n","\n","if not os.path.isfile( glove_100k_50d_path ): # если еще не загружали\n","    if not os.path.exists(glove_dir):\n","        os.makedirs(glove_dir)\n","    \n","    # пытаемся скачать файлы\n","    response = requests.get(download_url, stream=True)\n","    if response.status_code == requests.codes.ok:\n","        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n","        with open(glove_100k_50d_path, 'wb') as out_file:\n","            shutil.copyfileobj(response.raw, out_file)\n","    else:\n","        # если не получается, скачиваем другие версии файлов \n","        if not os.path.exists(data_cache):\n","            os.makedirs(data_cache)\n","        \n","        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n","            zipfilepath = os.path.join(data_cache, glove_full_tar)\n","            if not os.path.isfile( zipfilepath ):\n","                print(\"Downloading 860Mb GloVE file from Stanford\")\n","                response = requests.get(download_url, stream=True)\n","                with open(zipfilepath, 'wb') as out_file:\n","                    shutil.copyfileobj(response.raw, out_file)\n","            if os.path.isfile(zipfilepath):\n","                print(\"Unpacking 50d GloVE file from zip\")\n","                import zipfile\n","                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n","\n","        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n","            with open(glove_100k_50d_path, 'wt') as out_file:\n","                print(\"Reducing 50d GloVE file to first 100k words\")\n","                for i, l in enumerate(in_file.readlines()):\n","                    if i>=100000: break\n","                    out_file.write(l)\n","    \n","        # Get rid of tarfile source (the required text file itself will remain)\n","        #os.unlink(zipfilepath)\n","        #os.unlink(os.path.join(data_cache, glove_full_50d))\n","\n","print(\"GloVE available locally\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Downloading 42Mb pre-prepared GloVE file from RedCatLabs\n","GloVE available locally\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59rgDDHgLvaa","executionInfo":{"status":"ok","timestamp":1625396547542,"user_tz":-180,"elapsed":2282,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"30818bdf-e8b6-4264-d69c-503b72e3ab83"},"source":["# Ограничимся 100k наиболее частыми словами\n","word_embedding = glove.Glove.load_stanford( glove_100k_50d_path ) # загружаем в память\n","word_embedding.word_vectors.shape # размер матрицы"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100000, 50)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KB5xTURnMFGW","executionInfo":{"status":"ok","timestamp":1625396651599,"user_tz":-180,"elapsed":300,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"00bc1fe6-61e4-4d7a-c368-157f70df03ff"},"source":["word_embedding.dictionary # словарь"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'the': 0,\n"," ',': 1,\n"," '.': 2,\n"," 'of': 3,\n"," 'to': 4,\n"," 'and': 5,\n"," 'in': 6,\n"," 'a': 7,\n"," '\"': 8,\n"," \"'s\": 9,\n"," 'for': 10,\n"," '-': 11,\n"," 'that': 12,\n"," 'on': 13,\n"," 'is': 14,\n"," 'was': 15,\n"," 'said': 16,\n"," 'with': 17,\n"," 'he': 18,\n"," 'as': 19,\n"," 'it': 20,\n"," 'by': 21,\n"," 'at': 22,\n"," '(': 23,\n"," ')': 24,\n"," 'from': 25,\n"," 'his': 26,\n"," \"''\": 27,\n"," '``': 28,\n"," 'an': 29,\n"," 'be': 30,\n"," 'has': 31,\n"," 'are': 32,\n"," 'have': 33,\n"," 'but': 34,\n"," 'were': 35,\n"," 'not': 36,\n"," 'this': 37,\n"," 'who': 38,\n"," 'they': 39,\n"," 'had': 40,\n"," 'i': 41,\n"," 'which': 42,\n"," 'will': 43,\n"," 'their': 44,\n"," ':': 45,\n"," 'or': 46,\n"," 'its': 47,\n"," 'one': 48,\n"," 'after': 49,\n"," 'new': 50,\n"," 'been': 51,\n"," 'also': 52,\n"," 'we': 53,\n"," 'would': 54,\n"," 'two': 55,\n"," 'more': 56,\n"," \"'\": 57,\n"," 'first': 58,\n"," 'about': 59,\n"," 'up': 60,\n"," 'when': 61,\n"," 'year': 62,\n"," 'there': 63,\n"," 'all': 64,\n"," '--': 65,\n"," 'out': 66,\n"," 'she': 67,\n"," 'other': 68,\n"," 'people': 69,\n"," \"n't\": 70,\n"," 'her': 71,\n"," 'percent': 72,\n"," 'than': 73,\n"," 'over': 74,\n"," 'into': 75,\n"," 'last': 76,\n"," 'some': 77,\n"," 'government': 78,\n"," 'time': 79,\n"," '$': 80,\n"," 'you': 81,\n"," 'years': 82,\n"," 'if': 83,\n"," 'no': 84,\n"," 'world': 85,\n"," 'can': 86,\n"," 'three': 87,\n"," 'do': 88,\n"," ';': 89,\n"," 'president': 90,\n"," 'only': 91,\n"," 'state': 92,\n"," 'million': 93,\n"," 'could': 94,\n"," 'us': 95,\n"," 'most': 96,\n"," '_': 97,\n"," 'against': 98,\n"," 'u.s.': 99,\n"," 'so': 100,\n"," 'them': 101,\n"," 'what': 102,\n"," 'him': 103,\n"," 'united': 104,\n"," 'during': 105,\n"," 'before': 106,\n"," 'may': 107,\n"," 'since': 108,\n"," 'many': 109,\n"," 'while': 110,\n"," 'where': 111,\n"," 'states': 112,\n"," 'because': 113,\n"," 'now': 114,\n"," 'city': 115,\n"," 'made': 116,\n"," 'like': 117,\n"," 'between': 118,\n"," 'did': 119,\n"," 'just': 120,\n"," 'national': 121,\n"," 'day': 122,\n"," 'country': 123,\n"," 'under': 124,\n"," 'such': 125,\n"," 'second': 126,\n"," 'then': 127,\n"," 'company': 128,\n"," 'group': 129,\n"," 'any': 130,\n"," 'through': 131,\n"," 'china': 132,\n"," 'four': 133,\n"," 'being': 134,\n"," 'down': 135,\n"," 'war': 136,\n"," 'back': 137,\n"," 'off': 138,\n"," 'south': 139,\n"," 'american': 140,\n"," 'minister': 141,\n"," 'police': 142,\n"," 'well': 143,\n"," 'including': 144,\n"," 'team': 145,\n"," 'international': 146,\n"," 'week': 147,\n"," 'officials': 148,\n"," 'still': 149,\n"," 'both': 150,\n"," 'even': 151,\n"," 'high': 152,\n"," 'part': 153,\n"," 'told': 154,\n"," 'those': 155,\n"," 'end': 156,\n"," 'former': 157,\n"," 'these': 158,\n"," 'make': 159,\n"," 'billion': 160,\n"," 'work': 161,\n"," 'our': 162,\n"," 'home': 163,\n"," 'school': 164,\n"," 'party': 165,\n"," 'house': 166,\n"," 'old': 167,\n"," 'later': 168,\n"," 'get': 169,\n"," 'another': 170,\n"," 'tuesday': 171,\n"," 'news': 172,\n"," 'long': 173,\n"," 'five': 174,\n"," 'called': 175,\n"," '1': 176,\n"," 'wednesday': 177,\n"," 'military': 178,\n"," 'way': 179,\n"," 'used': 180,\n"," 'much': 181,\n"," 'next': 182,\n"," 'monday': 183,\n"," 'thursday': 184,\n"," 'friday': 185,\n"," 'game': 186,\n"," 'here': 187,\n"," '?': 188,\n"," 'should': 189,\n"," 'take': 190,\n"," 'very': 191,\n"," 'my': 192,\n"," 'north': 193,\n"," 'security': 194,\n"," 'season': 195,\n"," 'york': 196,\n"," 'how': 197,\n"," 'public': 198,\n"," 'early': 199,\n"," 'according': 200,\n"," 'several': 201,\n"," 'court': 202,\n"," 'say': 203,\n"," 'around': 204,\n"," 'foreign': 205,\n"," '10': 206,\n"," 'until': 207,\n"," 'set': 208,\n"," 'political': 209,\n"," 'says': 210,\n"," 'market': 211,\n"," 'however': 212,\n"," 'family': 213,\n"," 'life': 214,\n"," 'same': 215,\n"," 'general': 216,\n"," '–': 217,\n"," 'left': 218,\n"," 'good': 219,\n"," 'top': 220,\n"," 'university': 221,\n"," 'going': 222,\n"," 'number': 223,\n"," 'major': 224,\n"," 'known': 225,\n"," 'points': 226,\n"," 'won': 227,\n"," 'six': 228,\n"," 'month': 229,\n"," 'dollars': 230,\n"," 'bank': 231,\n"," '2': 232,\n"," 'iraq': 233,\n"," 'use': 234,\n"," 'members': 235,\n"," 'each': 236,\n"," 'area': 237,\n"," 'found': 238,\n"," 'official': 239,\n"," 'sunday': 240,\n"," 'place': 241,\n"," 'go': 242,\n"," 'based': 243,\n"," 'among': 244,\n"," 'third': 245,\n"," 'times': 246,\n"," 'took': 247,\n"," 'right': 248,\n"," 'days': 249,\n"," 'local': 250,\n"," 'economic': 251,\n"," 'countries': 252,\n"," 'see': 253,\n"," 'best': 254,\n"," 'report': 255,\n"," 'killed': 256,\n"," 'held': 257,\n"," 'business': 258,\n"," 'west': 259,\n"," 'does': 260,\n"," 'own': 261,\n"," '%': 262,\n"," 'came': 263,\n"," 'law': 264,\n"," 'months': 265,\n"," 'women': 266,\n"," \"'re\": 267,\n"," 'power': 268,\n"," 'think': 269,\n"," 'service': 270,\n"," 'children': 271,\n"," 'bush': 272,\n"," 'show': 273,\n"," '/': 274,\n"," 'help': 275,\n"," 'chief': 276,\n"," 'saturday': 277,\n"," 'system': 278,\n"," 'john': 279,\n"," 'support': 280,\n"," 'series': 281,\n"," 'play': 282,\n"," 'office': 283,\n"," 'following': 284,\n"," 'me': 285,\n"," 'meeting': 286,\n"," 'expected': 287,\n"," 'late': 288,\n"," 'washington': 289,\n"," 'games': 290,\n"," 'european': 291,\n"," 'league': 292,\n"," 'reported': 293,\n"," 'final': 294,\n"," 'added': 295,\n"," 'without': 296,\n"," 'british': 297,\n"," 'white': 298,\n"," 'history': 299,\n"," 'man': 300,\n"," 'men': 301,\n"," 'became': 302,\n"," 'want': 303,\n"," 'march': 304,\n"," 'case': 305,\n"," 'few': 306,\n"," 'run': 307,\n"," 'money': 308,\n"," 'began': 309,\n"," 'open': 310,\n"," 'name': 311,\n"," 'trade': 312,\n"," 'center': 313,\n"," '3': 314,\n"," 'israel': 315,\n"," 'oil': 316,\n"," 'too': 317,\n"," 'al': 318,\n"," 'film': 319,\n"," 'win': 320,\n"," 'led': 321,\n"," 'east': 322,\n"," 'central': 323,\n"," '20': 324,\n"," 'air': 325,\n"," 'come': 326,\n"," 'chinese': 327,\n"," 'town': 328,\n"," 'leader': 329,\n"," 'army': 330,\n"," 'line': 331,\n"," 'never': 332,\n"," 'little': 333,\n"," 'played': 334,\n"," 'prime': 335,\n"," 'death': 336,\n"," 'companies': 337,\n"," 'least': 338,\n"," 'put': 339,\n"," 'forces': 340,\n"," 'past': 341,\n"," 'de': 342,\n"," 'half': 343,\n"," 'june': 344,\n"," 'saying': 345,\n"," 'know': 346,\n"," 'federal': 347,\n"," 'french': 348,\n"," 'peace': 349,\n"," 'earlier': 350,\n"," 'capital': 351,\n"," 'force': 352,\n"," 'great': 353,\n"," 'union': 354,\n"," 'near': 355,\n"," 'released': 356,\n"," 'small': 357,\n"," 'department': 358,\n"," 'every': 359,\n"," 'health': 360,\n"," 'japan': 361,\n"," 'head': 362,\n"," 'ago': 363,\n"," 'night': 364,\n"," 'big': 365,\n"," 'cup': 366,\n"," 'election': 367,\n"," 'region': 368,\n"," 'director': 369,\n"," 'talks': 370,\n"," 'program': 371,\n"," 'far': 372,\n"," 'today': 373,\n"," 'statement': 374,\n"," 'july': 375,\n"," 'although': 376,\n"," 'district': 377,\n"," 'again': 378,\n"," 'born': 379,\n"," 'development': 380,\n"," 'leaders': 381,\n"," 'council': 382,\n"," 'close': 383,\n"," 'record': 384,\n"," 'along': 385,\n"," 'county': 386,\n"," 'france': 387,\n"," 'went': 388,\n"," 'point': 389,\n"," 'must': 390,\n"," 'spokesman': 391,\n"," 'your': 392,\n"," 'member': 393,\n"," 'plan': 394,\n"," 'financial': 395,\n"," 'april': 396,\n"," 'recent': 397,\n"," 'campaign': 398,\n"," 'become': 399,\n"," 'troops': 400,\n"," 'whether': 401,\n"," 'lost': 402,\n"," 'music': 403,\n"," '15': 404,\n"," 'got': 405,\n"," 'israeli': 406,\n"," '30': 407,\n"," 'need': 408,\n"," '4': 409,\n"," 'lead': 410,\n"," 'already': 411,\n"," 'russia': 412,\n"," 'though': 413,\n"," 'might': 414,\n"," 'free': 415,\n"," 'hit': 416,\n"," 'rights': 417,\n"," '11': 418,\n"," 'information': 419,\n"," 'away': 420,\n"," '12': 421,\n"," '5': 422,\n"," 'others': 423,\n"," 'control': 424,\n"," 'within': 425,\n"," 'large': 426,\n"," 'economy': 427,\n"," 'press': 428,\n"," 'agency': 429,\n"," 'water': 430,\n"," 'died': 431,\n"," 'career': 432,\n"," 'making': 433,\n"," '...': 434,\n"," 'deal': 435,\n"," 'attack': 436,\n"," 'side': 437,\n"," 'seven': 438,\n"," 'better': 439,\n"," 'less': 440,\n"," 'september': 441,\n"," 'once': 442,\n"," 'clinton': 443,\n"," 'main': 444,\n"," 'due': 445,\n"," 'committee': 446,\n"," 'building': 447,\n"," 'conference': 448,\n"," 'club': 449,\n"," 'january': 450,\n"," 'decision': 451,\n"," 'stock': 452,\n"," 'america': 453,\n"," 'given': 454,\n"," 'give': 455,\n"," 'often': 456,\n"," 'announced': 457,\n"," 'television': 458,\n"," 'industry': 459,\n"," 'order': 460,\n"," 'young': 461,\n"," \"'ve\": 462,\n"," 'palestinian': 463,\n"," 'age': 464,\n"," 'start': 465,\n"," 'administration': 466,\n"," 'russian': 467,\n"," 'prices': 468,\n"," 'round': 469,\n"," 'december': 470,\n"," 'nations': 471,\n"," \"'m\": 472,\n"," 'human': 473,\n"," 'india': 474,\n"," 'defense': 475,\n"," 'asked': 476,\n"," 'total': 477,\n"," 'october': 478,\n"," 'players': 479,\n"," 'bill': 480,\n"," 'important': 481,\n"," 'southern': 482,\n"," 'move': 483,\n"," 'fire': 484,\n"," 'population': 485,\n"," 'rose': 486,\n"," 'november': 487,\n"," 'include': 488,\n"," 'further': 489,\n"," 'nuclear': 490,\n"," 'street': 491,\n"," 'taken': 492,\n"," 'media': 493,\n"," 'different': 494,\n"," 'issue': 495,\n"," 'received': 496,\n"," 'secretary': 497,\n"," 'return': 498,\n"," 'college': 499,\n"," 'working': 500,\n"," 'community': 501,\n"," 'eight': 502,\n"," 'groups': 503,\n"," 'despite': 504,\n"," 'level': 505,\n"," 'largest': 506,\n"," 'whose': 507,\n"," 'attacks': 508,\n"," 'germany': 509,\n"," 'august': 510,\n"," 'change': 511,\n"," 'church': 512,\n"," 'nation': 513,\n"," 'german': 514,\n"," 'station': 515,\n"," 'london': 516,\n"," 'weeks': 517,\n"," 'having': 518,\n"," '18': 519,\n"," 'research': 520,\n"," 'black': 521,\n"," 'services': 522,\n"," 'story': 523,\n"," '6': 524,\n"," 'europe': 525,\n"," 'sales': 526,\n"," 'policy': 527,\n"," 'visit': 528,\n"," 'northern': 529,\n"," 'lot': 530,\n"," 'across': 531,\n"," 'per': 532,\n"," 'current': 533,\n"," 'board': 534,\n"," 'football': 535,\n"," 'ministry': 536,\n"," 'workers': 537,\n"," 'vote': 538,\n"," 'book': 539,\n"," 'fell': 540,\n"," 'seen': 541,\n"," 'role': 542,\n"," 'students': 543,\n"," 'shares': 544,\n"," 'iran': 545,\n"," 'process': 546,\n"," 'agreement': 547,\n"," 'quarter': 548,\n"," 'full': 549,\n"," 'match': 550,\n"," 'started': 551,\n"," 'growth': 552,\n"," 'yet': 553,\n"," 'moved': 554,\n"," 'possible': 555,\n"," 'western': 556,\n"," 'special': 557,\n"," '100': 558,\n"," 'plans': 559,\n"," 'interest': 560,\n"," 'behind': 561,\n"," 'strong': 562,\n"," 'england': 563,\n"," 'named': 564,\n"," 'food': 565,\n"," 'period': 566,\n"," 'real': 567,\n"," 'authorities': 568,\n"," 'car': 569,\n"," 'term': 570,\n"," 'rate': 571,\n"," 'race': 572,\n"," 'nearly': 573,\n"," 'korea': 574,\n"," 'enough': 575,\n"," 'site': 576,\n"," 'opposition': 577,\n"," 'keep': 578,\n"," '25': 579,\n"," 'call': 580,\n"," 'future': 581,\n"," 'taking': 582,\n"," 'island': 583,\n"," '2008': 584,\n"," '2006': 585,\n"," 'road': 586,\n"," 'outside': 587,\n"," 'really': 588,\n"," 'century': 589,\n"," 'democratic': 590,\n"," 'almost': 591,\n"," 'single': 592,\n"," 'share': 593,\n"," 'leading': 594,\n"," 'trying': 595,\n"," 'find': 596,\n"," 'album': 597,\n"," 'senior': 598,\n"," 'minutes': 599,\n"," 'together': 600,\n"," 'congress': 601,\n"," 'index': 602,\n"," 'australia': 603,\n"," 'results': 604,\n"," 'hard': 605,\n"," 'hours': 606,\n"," 'land': 607,\n"," 'action': 608,\n"," 'higher': 609,\n"," 'field': 610,\n"," 'cut': 611,\n"," 'coach': 612,\n"," 'elections': 613,\n"," 'san': 614,\n"," 'issues': 615,\n"," 'executive': 616,\n"," 'february': 617,\n"," 'production': 618,\n"," 'areas': 619,\n"," 'river': 620,\n"," 'face': 621,\n"," 'using': 622,\n"," 'japanese': 623,\n"," 'province': 624,\n"," 'park': 625,\n"," 'price': 626,\n"," 'commission': 627,\n"," 'california': 628,\n"," 'father': 629,\n"," 'son': 630,\n"," 'education': 631,\n"," '7': 632,\n"," 'village': 633,\n"," 'energy': 634,\n"," 'shot': 635,\n"," 'short': 636,\n"," 'africa': 637,\n"," 'key': 638,\n"," 'red': 639,\n"," 'association': 640,\n"," 'average': 641,\n"," 'pay': 642,\n"," 'exchange': 643,\n"," 'eu': 644,\n"," 'something': 645,\n"," 'gave': 646,\n"," 'likely': 647,\n"," 'player': 648,\n"," 'george': 649,\n"," '2007': 650,\n"," 'victory': 651,\n"," '8': 652,\n"," 'low': 653,\n"," 'things': 654,\n"," '2010': 655,\n"," 'pakistan': 656,\n"," '14': 657,\n"," 'post': 658,\n"," 'social': 659,\n"," 'continue': 660,\n"," 'ever': 661,\n"," 'look': 662,\n"," 'chairman': 663,\n"," 'job': 664,\n"," '2000': 665,\n"," 'soldiers': 666,\n"," 'able': 667,\n"," 'parliament': 668,\n"," 'front': 669,\n"," 'himself': 670,\n"," 'problems': 671,\n"," 'private': 672,\n"," 'lower': 673,\n"," 'list': 674,\n"," 'built': 675,\n"," '13': 676,\n"," 'efforts': 677,\n"," 'dollar': 678,\n"," 'miles': 679,\n"," 'included': 680,\n"," 'radio': 681,\n"," 'live': 682,\n"," 'form': 683,\n"," 'david': 684,\n"," 'african': 685,\n"," 'increase': 686,\n"," 'reports': 687,\n"," 'sent': 688,\n"," 'fourth': 689,\n"," 'always': 690,\n"," 'king': 691,\n"," '50': 692,\n"," 'tax': 693,\n"," 'taiwan': 694,\n"," 'britain': 695,\n"," '16': 696,\n"," 'playing': 697,\n"," 'title': 698,\n"," 'middle': 699,\n"," 'meet': 700,\n"," 'global': 701,\n"," 'wife': 702,\n"," '2009': 703,\n"," 'position': 704,\n"," 'located': 705,\n"," 'clear': 706,\n"," 'ahead': 707,\n"," '2004': 708,\n"," '2005': 709,\n"," 'iraqi': 710,\n"," 'english': 711,\n"," 'result': 712,\n"," 'release': 713,\n"," 'violence': 714,\n"," 'goal': 715,\n"," 'project': 716,\n"," 'closed': 717,\n"," 'border': 718,\n"," 'body': 719,\n"," 'soon': 720,\n"," 'crisis': 721,\n"," 'division': 722,\n"," '&amp;': 723,\n"," 'served': 724,\n"," 'tour': 725,\n"," 'hospital': 726,\n"," 'kong': 727,\n"," 'test': 728,\n"," 'hong': 729,\n"," 'u.n.': 730,\n"," 'inc.': 731,\n"," 'technology': 732,\n"," 'believe': 733,\n"," 'organization': 734,\n"," 'published': 735,\n"," 'weapons': 736,\n"," 'agreed': 737,\n"," 'why': 738,\n"," 'nine': 739,\n"," 'summer': 740,\n"," 'wanted': 741,\n"," 'republican': 742,\n"," 'act': 743,\n"," 'recently': 744,\n"," 'texas': 745,\n"," 'course': 746,\n"," 'problem': 747,\n"," 'senate': 748,\n"," 'medical': 749,\n"," 'un': 750,\n"," 'done': 751,\n"," 'reached': 752,\n"," 'star': 753,\n"," 'continued': 754,\n"," 'investors': 755,\n"," 'living': 756,\n"," 'care': 757,\n"," 'signed': 758,\n"," '17': 759,\n"," 'art': 760,\n"," 'provide': 761,\n"," 'worked': 762,\n"," 'presidential': 763,\n"," 'gold': 764,\n"," 'obama': 765,\n"," 'morning': 766,\n"," 'dead': 767,\n"," 'opened': 768,\n"," \"'ll\": 769,\n"," 'event': 770,\n"," 'previous': 771,\n"," 'cost': 772,\n"," 'instead': 773,\n"," 'canada': 774,\n"," 'band': 775,\n"," 'teams': 776,\n"," 'daily': 777,\n"," '2001': 778,\n"," 'available': 779,\n"," 'drug': 780,\n"," 'coming': 781,\n"," '2003': 782,\n"," 'investment': 783,\n"," '’s': 784,\n"," 'michael': 785,\n"," 'civil': 786,\n"," 'woman': 787,\n"," 'training': 788,\n"," 'appeared': 789,\n"," '9': 790,\n"," 'involved': 791,\n"," 'indian': 792,\n"," 'similar': 793,\n"," 'situation': 794,\n"," '24': 795,\n"," 'los': 796,\n"," 'running': 797,\n"," 'fighting': 798,\n"," 'mark': 799,\n"," '40': 800,\n"," 'trial': 801,\n"," 'hold': 802,\n"," 'australian': 803,\n"," 'thought': 804,\n"," '!': 805,\n"," 'study': 806,\n"," 'fall': 807,\n"," 'mother': 808,\n"," 'met': 809,\n"," 'relations': 810,\n"," 'anti': 811,\n"," '2002': 812,\n"," 'song': 813,\n"," 'popular': 814,\n"," 'base': 815,\n"," 'tv': 816,\n"," 'ground': 817,\n"," 'markets': 818,\n"," 'ii': 819,\n"," 'newspaper': 820,\n"," 'staff': 821,\n"," 'saw': 822,\n"," 'hand': 823,\n"," 'hope': 824,\n"," 'operations': 825,\n"," 'pressure': 826,\n"," 'americans': 827,\n"," 'eastern': 828,\n"," 'st.': 829,\n"," 'legal': 830,\n"," 'asia': 831,\n"," 'budget': 832,\n"," 'returned': 833,\n"," 'considered': 834,\n"," 'love': 835,\n"," 'wrote': 836,\n"," 'stop': 837,\n"," 'fight': 838,\n"," 'currently': 839,\n"," 'charges': 840,\n"," 'try': 841,\n"," 'aid': 842,\n"," 'ended': 843,\n"," 'management': 844,\n"," 'brought': 845,\n"," 'cases': 846,\n"," 'decided': 847,\n"," 'failed': 848,\n"," 'network': 849,\n"," 'works': 850,\n"," 'gas': 851,\n"," 'turned': 852,\n"," 'fact': 853,\n"," 'vice': 854,\n"," 'ca': 855,\n"," 'mexico': 856,\n"," 'trading': 857,\n"," 'especially': 858,\n"," 'reporters': 859,\n"," 'afghanistan': 860,\n"," 'common': 861,\n"," 'looking': 862,\n"," 'space': 863,\n"," 'rates': 864,\n"," 'manager': 865,\n"," 'loss': 866,\n"," '2011': 867,\n"," 'justice': 868,\n"," 'thousands': 869,\n"," 'james': 870,\n"," 'rather': 871,\n"," 'fund': 872,\n"," 'thing': 873,\n"," 'republic': 874,\n"," 'opening': 875,\n"," 'accused': 876,\n"," 'winning': 877,\n"," 'scored': 878,\n"," 'championship': 879,\n"," 'example': 880,\n"," 'getting': 881,\n"," 'biggest': 882,\n"," 'performance': 883,\n"," 'sports': 884,\n"," '1998': 885,\n"," 'let': 886,\n"," 'allowed': 887,\n"," 'schools': 888,\n"," 'means': 889,\n"," 'turn': 890,\n"," 'leave': 891,\n"," 'no.': 892,\n"," 'robert': 893,\n"," 'personal': 894,\n"," 'stocks': 895,\n"," 'showed': 896,\n"," 'light': 897,\n"," 'arrested': 898,\n"," 'person': 899,\n"," 'either': 900,\n"," 'offer': 901,\n"," 'majority': 902,\n"," 'battle': 903,\n"," '19': 904,\n"," 'class': 905,\n"," 'evidence': 906,\n"," 'makes': 907,\n"," 'society': 908,\n"," 'products': 909,\n"," 'regional': 910,\n"," 'needed': 911,\n"," 'stage': 912,\n"," 'am': 913,\n"," 'doing': 914,\n"," 'families': 915,\n"," 'construction': 916,\n"," 'various': 917,\n"," '1996': 918,\n"," 'sold': 919,\n"," 'independent': 920,\n"," 'kind': 921,\n"," 'airport': 922,\n"," 'paul': 923,\n"," 'judge': 924,\n"," 'internet': 925,\n"," 'movement': 926,\n"," 'room': 927,\n"," 'followed': 928,\n"," 'original': 929,\n"," 'angeles': 930,\n"," 'italy': 931,\n"," '`': 932,\n"," 'data': 933,\n"," 'comes': 934,\n"," 'parties': 935,\n"," 'nothing': 936,\n"," 'sea': 937,\n"," 'bring': 938,\n"," '2012': 939,\n"," 'annual': 940,\n"," 'officer': 941,\n"," 'beijing': 942,\n"," 'present': 943,\n"," 'remain': 944,\n"," 'nato': 945,\n"," '1999': 946,\n"," '22': 947,\n"," 'remains': 948,\n"," 'allow': 949,\n"," 'florida': 950,\n"," 'computer': 951,\n"," '21': 952,\n"," 'contract': 953,\n"," 'coast': 954,\n"," 'created': 955,\n"," 'demand': 956,\n"," 'operation': 957,\n"," 'events': 958,\n"," 'islamic': 959,\n"," 'beat': 960,\n"," 'analysts': 961,\n"," 'interview': 962,\n"," 'helped': 963,\n"," 'child': 964,\n"," 'probably': 965,\n"," 'spent': 966,\n"," 'asian': 967,\n"," 'effort': 968,\n"," 'cooperation': 969,\n"," 'shows': 970,\n"," 'calls': 971,\n"," 'investigation': 972,\n"," 'lives': 973,\n"," 'video': 974,\n"," 'yen': 975,\n"," 'runs': 976,\n"," 'tried': 977,\n"," 'bad': 978,\n"," 'described': 979,\n"," '1994': 980,\n"," 'toward': 981,\n"," 'written': 982,\n"," 'throughout': 983,\n"," 'established': 984,\n"," 'mission': 985,\n"," 'associated': 986,\n"," 'buy': 987,\n"," 'growing': 988,\n"," 'green': 989,\n"," 'forward': 990,\n"," 'competition': 991,\n"," 'poor': 992,\n"," 'latest': 993,\n"," 'banks': 994,\n"," 'question': 995,\n"," '1997': 996,\n"," 'prison': 997,\n"," 'feel': 998,\n"," 'attention': 999,\n"," ...}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"v6DbHa8NMOM1"},"source":["Проверяем работу векторов:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ta64e3v9MRD9","executionInfo":{"status":"ok","timestamp":1625396689626,"user_tz":-180,"elapsed":292,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"9308eb41-c9a5-49bb-897d-ec9819efc7f8"},"source":["# Наиболее похожее слово\n","word_embedding.most_similar('country')"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('nation', 0.9162598787943158),\n"," ('bringing', 0.8718220802078342),\n"," ('now', 0.838748566812629),\n"," ('countries', 0.8238734388492914)]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"UJ92F_2NMWKp","executionInfo":{"status":"ok","timestamp":1625397211953,"user_tz":-180,"elapsed":8,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["import numpy as np\n","# получаем вектор для слова (приводим к нижнему регистру)\n","def get_embedding_vec(word):\n","    idx = word_embedding.dictionary.get(word.lower(), -1)\n","    if idx<0:\n","        #print(\"Missing word : '%s'\" % (word,))\n","        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n","    return word_embedding.word_vectors[idx]\n","# ищем ближайшее слово\n","def get_closest_word(vec, number=5):\n","    # считаем косинусное расстояние от заданных векторов до всех векторов словаря\n","    dst = (np.dot(word_embedding.word_vectors, vec)\n","                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n","                   / np.linalg.norm(vec))\n","    word_ids = np.argsort(-dst) # сортируем по убыванию расстояния\n","    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n","            if x in word_embedding.inverse_dictionary]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUoMWy9QNM0l","executionInfo":{"status":"ok","timestamp":1625397215284,"user_tz":-180,"elapsed":276,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"6e7dfc64-11b3-427a-a2c0-d8013e6be9db"},"source":["# Аналогии слов: woman+king-man=?\n","analogy_vec = get_embedding_vec('woman') + get_embedding_vec('king') - get_embedding_vec('man')\n","get_closest_word(analogy_vec)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('king', 0.8859834623625933),\n"," ('queen', 0.8609581258578943),\n"," ('daughter', 0.768451180089547),\n"," ('prince', 0.764069959135472),\n"," ('throne', 0.7634970756412145)]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"V2fRzEEYOUxj"},"source":["В этом примере опять queen лишь второе, а king первое. Чтобы избежать таких ситуаций сами слова, входящие в выражение не учитывают при поиске результата.  "]},{"cell_type":"code","metadata":{"id":"O9I_kpcfPA5C","executionInfo":{"status":"ok","timestamp":1625397446977,"user_tz":-180,"elapsed":293,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["# аналогии слов\n","def test_analogy(s='one two three four'):\n","    (a,b,c,d) = s.split(' ')\n","    analogy_vec = get_embedding_vec(b) - get_embedding_vec(a) + get_embedding_vec(c)\n","    words = [ w for (w,p) in get_closest_word(analogy_vec) if w not in (a,b,c)] # не учитываем слова из выражения\n","    print(\"'%s' is to '%s' as '%s' is to {%s}\" % (a,b,c,', '.join(words)))"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HpfoR8lPObu","executionInfo":{"status":"ok","timestamp":1625397457816,"user_tz":-180,"elapsed":251,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"1be47c67-b8d2-4795-8a2d-652b1e8c624f"},"source":["test_analogy('man woman king queen')\n","test_analogy('paris france rome italy')\n","test_analogy('kitten cat puppy dog')\n","test_analogy('understand understood run ran')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["'man' is to 'woman' as 'king' is to {queen, daughter, prince, throne}\n","'paris' is to 'france' as 'rome' is to {italy, spain, portugal}\n","'kitten' is to 'cat' as 'puppy' is to {dog, rabbit, horse}\n","'understand' is to 'understood' as 'run' is to {ran, running, runs, twice}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O1LKUcw-PTWy"},"source":["Стало гораздо лучше! Попробуйте другие аналогии слов, найдите такие, которые не получаются в векторах."]},{"cell_type":"markdown","metadata":{"id":"OfGpQ7dlPrQn"},"source":["# Векторные представления FastText\n","В предыдущих представлениях слово должно быть в словаре, чтобы можно было найти для него вектор. Но что делать, если слова нет в словаре? \n","\n","Зададимся вопросом, что такое слово? \n","\n","В векторном представлении \"слово\" это элемент словаря, а значит им может быть все что угодно: смайлики, картинки, знаки пунктуации, словосочетания и т.п.\n","\n","Набор букв (символов) тоже может быть элементом словаря. А давайте все тексты разобьем на последовательности символов определенной длинны (их называют n-граммы) и будем создавать вектора для этих последовательностей.\n","\n","Реальное слово можно представить несколькими такими n-граммами, объединим (сложим) вектора для них и получим вектор для настоящего слова.\n","\n","Никто не запрещает иметь в словаре одновременно и настоящие слова и их n-граммы. \n","\n","Но это значит что для любого слова, которого даже нет в словаре, мы можем построить вектор из векторов его n-грамм. Такой подход реализован, например, в [FastText](https://amitness.com/2020/06/fasttext-embeddings/), доступна в библиотеке gensim (но ее нужно обновить и перезапустить тетрадку).\n","\n","![img](https://amitness.com/images/fasttext-center-word-embedding.png)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"PhEAweXBQehA","executionInfo":{"status":"ok","timestamp":1625397835969,"user_tz":-180,"elapsed":7759,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"4297ac59-f9e0-49ba-cd04-da7b6048adba"},"source":["!pip install --upgrade gensim"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Collecting gensim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n","\u001b[K     |████████████████████████████████| 23.9MB 111kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n","Installing collected packages: gensim\n","  Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.0.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gensim"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mas1S5YwQZvI","executionInfo":{"status":"ok","timestamp":1625397868835,"user_tz":-180,"elapsed":1342,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"ac8b0fe3-fbc2-40ca-e414-6b3e690b4efa"},"source":["from gensim.models import fasttext\n","from gensim.test.utils import datapath\n","\n","cap_path = datapath(\"crime-and-punishment.bin\") # вектора полученные по какому-то тексту  \n","\n","wv = fasttext.load_facebook_vectors(cap_path) # загружаем их\n","\n","print('landlord' in wv.key_to_index)  # проверяем есть ли такое слово в словаре\n","print(wv['landlord'])  # его нет, а вектор для него можем построить\n","\n","\n","print('хотел' in wv.key_to_index)  # а это слово есть в словаре\n","print(wv['хотел']) # и конечно вектор его"],"execution_count":1,"outputs":[{"output_type":"stream","text":["False\n","[-0.05853396 -0.00144831  0.00096381  0.09085083  0.08532218]\n","True\n","[-0.00507655  0.03531152  0.0417647   0.02530364  0.01639834]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n","  warnings.warn(msg)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBzi_YpzRqOT","executionInfo":{"status":"ok","timestamp":1625398133930,"user_tz":-180,"elapsed":267,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"e4964c75-8eaa-48e1-e9ec-b4e0a40dc75e"},"source":["print(wv['абракадабра111___)))']) # даже для такой ерунды есть вектор"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[-0.03246168  0.04171836  0.03426896  0.08179154  0.03010553]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VDKZihnmRFa2"},"source":["Здесь текст для обучения был маленьким (отрывок из Войны и мира), поэтому аналогии слов не очень понятны, но работают. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFBC1XRoQ_e9","executionInfo":{"status":"ok","timestamp":1625397920957,"user_tz":-180,"elapsed":270,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"23c4e3c0-d78a-43b9-8fa8-0b1d699c57e1"},"source":["# похожие слова\n","similarities = wv.most_similar(positive=['time'])\n","print(similarities)\n","\n","# не связанное слово\n","not_matching = wv.doesnt_match(\"human computer interface tree\".split())\n","print(not_matching)\n","\n","# похожесть слов\n","sim_score = wv.similarity('computer', 'human')\n","print(sim_score)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[('under', 0.990774393081665), ('нерешимости,', 0.9880985617637634), ('останавливаться', 0.9870225191116333), ('лестницей', 0.9839380383491516), ('чувствовал', 0.9788582921028137), ('лестницу.', 0.9766556620597839), ('самою', 0.975305438041687), ('время,', 0.9742954969406128), ('переулке,', 0.9740443229675293), ('жаркое', 0.9740228056907654)]\n","tree\n","0.9894072\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEnKTRphSCY5","executionInfo":{"status":"ok","timestamp":1625398214781,"user_tz":-180,"elapsed":745,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"095a0d78-595b-496b-fd68-5e487dbfaf60"},"source":["wv.key_to_index # словарь"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'--': 17,\n"," '</s>': 8,\n"," 'And': 99,\n"," 'He': 21,\n"," 'His': 130,\n"," 'July': 60,\n"," 'K.': 140,\n"," 'On': 53,\n"," 'Place': 133,\n"," 'S.': 132,\n"," 'The': 156,\n"," 'a': 9,\n"," 'afraid': 128,\n"," 'an': 51,\n"," 'and': 4,\n"," 'as': 136,\n"," 'ashamed.': 124,\n"," 'attendance,': 101,\n"," 'avoided': 143,\n"," 'below,': 104,\n"," 'bridge.': 141,\n"," 'came': 145,\n"," 'cupboard': 153,\n"," 'debt': 126,\n"," 'dinners,': 100,\n"," 'door': 111,\n"," 'each': 116,\n"," 'early': 59,\n"," 'evening': 58,\n"," 'every': 105,\n"," 'exceptionally': 54,\n"," 'feel': 123,\n"," 'feeling,': 120,\n"," 'five-storied': 149,\n"," 'floor': 103,\n"," 'frightened': 119,\n"," 'garret': 42,\n"," 'garret,': 115,\n"," 'had': 25,\n"," 'he': 14,\n"," 'her': 109,\n"," 'her.': 129,\n"," 'hesitation,': 138,\n"," 'high,': 148,\n"," 'him': 28,\n"," 'his': 29,\n"," 'hopelessly': 125,\n"," 'hot': 57,\n"," 'house': 150,\n"," 'in': 10,\n"," 'invariably': 112,\n"," 'kitchen,': 110,\n"," 'landlady': 30,\n"," 'landlady,': 127,\n"," 'like': 152,\n"," 'lived': 102,\n"," 'lodged': 131,\n"," 'made': 121,\n"," 'man': 37,\n"," 'meeting': 27,\n"," 'more': 151,\n"," 'obliged': 107,\n"," 'of': 15,\n"," 'on': 20,\n"," 'open.': 114,\n"," 'out': 23,\n"," 'pass': 108,\n"," 'passed,': 117,\n"," 'provided': 158,\n"," 'roof': 147,\n"," 'room.': 155,\n"," 'scowl': 122,\n"," 'sick,': 118,\n"," 'slowly,': 135,\n"," 'staircase.': 144,\n"," 'stood': 113,\n"," 'successfully': 142,\n"," 'than': 154,\n"," 'the': 3,\n"," 'though': 137,\n"," 'time': 48,\n"," 'to': 45,\n"," 'towards': 139,\n"," 'under': 146,\n"," 'walked': 134,\n"," 'was': 5,\n"," 'went': 106,\n"," 'which': 19,\n"," 'who': 157,\n"," 'with': 159,\n"," 'young': 35,\n"," 'В': 245,\n"," 'И': 267,\n"," 'К': 217,\n"," 'Каморка': 225,\n"," 'Квартирная': 165,\n"," 'Насущными': 251,\n"," 'Не': 284,\n"," 'Никакой': 257,\n"," 'Но': 73,\n"," 'Он': 12,\n"," 'С': 195,\n"," 'бедностью;': 235,\n"," 'благополучно': 220,\n"," 'более': 160,\n"," 'болезненное': 274,\n"," 'боялся': 46,\n"," 'боялся,': 89,\n"," 'бы': 38,\n"," 'был': 13,\n"," 'было': 191,\n"," 'в': 1,\n"," 'вечер,': 204,\n"," 'вздор': 68,\n"," 'видал.': 50,\n"," 'времени': 242,\n"," 'время': 248,\n"," 'время,': 205,\n"," 'все': 71,\n"," 'всегда': 263,\n"," 'всех,': 232,\n"," 'встретиться.': 283,\n"," 'встречи': 22,\n"," 'встречи,': 230,\n"," 'всю': 66,\n"," 'всякий': 69,\n"," 'всякой': 231,\n"," 'высокого': 173,\n"," 'выходе': 187,\n"," 'вышел': 201,\n"," 'даже': 18,\n"," 'дела,': 62,\n"," 'делами': 252,\n"," 'до': 26,\n"," 'должен': 279,\n"," 'дома': 171,\n"," 'дребедень,': 64,\n"," 'его': 177,\n"," 'его,': 162,\n"," 'его.': 250,\n"," 'ему': 43,\n"," 'жалобы,': 93,\n"," 'жаркое': 206,\n"," 'же': 164,\n"," 'жильцов': 196,\n"," 'забит,': 289,\n"," 'задавлен': 227,\n"," 'замышляла': 76,\n"," 'заниматься.': 256,\n"," 'и': 0,\n"," 'из': 200,\n"," 'избегнул': 221,\n"," 'извиняться,': 52,\n"," 'изворачиваться,': 80,\n"," 'ипохондрию.': 237,\n"," 'июля,': 208,\n"," 'к': 216,\n"," 'каждый': 24,\n"," 'как': 213,\n"," 'как-нибудь': 84,\n"," 'какое-то': 273,\n"," 'каморки,': 198,\n"," 'каморку': 169,\n"," 'квартире,': 185,\n"," 'квартиру.': 166,\n"," 'которого': 44,\n"," 'которой': 36,\n"," 'которую': 197,\n"," 'кошкой': 83,\n"," 'кровлей': 174,\n"," 'кругом': 280,\n"," 'кухни,': 261,\n"," 'лгать,': 88,\n"," 'лестнице': 81,\n"," 'лестнице,': 61,\n"," 'лестнице.': 224,\n"," 'лестницей': 182,\n"," 'лестницу.': 266,\n"," 'лучше': 86,\n"," 'м': 194,\n"," 'медленно,': 212,\n"," 'мимо': 210,\n"," 'мимо,': 271,\n"," 'молодой': 40,\n"," 'морщился.': 278,\n"," 'мосту.': 219,\n"," 'на': 2,\n"," 'надо': 190,\n"," 'нанимал': 39,\n"," 'напротив;': 290,\n"," 'напряженном': 240,\n"," 'настежь': 264,\n"," 'начале': 209,\n"," 'не': 11,\n"," 'него.': 74,\n"," 'некоторого': 243,\n"," 'непременно': 189,\n"," 'нерешимости,': 214,\n"," 'нет': 41,\n"," 'нею': 282,\n"," 'ни': 77,\n"," 'ниже,': 183,\n"," 'никакого': 63,\n"," 'никто': 55,\n"," 'но': 32,\n"," 'ну': 218,\n"," 'о': 96,\n"," 'обедом': 178,\n"," 'обыденную': 65,\n"," 'один': 203,\n"," 'одною': 181,\n"," 'он': 6,\n"," 'останавливаться': 72,\n"," 'от': 16,\n"," 'отворенной': 265,\n"," 'отдельной': 184,\n"," 'отправился': 215,\n"," 'ощущение,': 276,\n"," 'перестал': 254,\n"," 'перестало': 246,\n"," 'переулке,': 202,\n"," 'платеже,': 95,\n"," 'по': 82,\n"," 'под': 49,\n"," 'положение': 79,\n"," 'помещалась': 180,\n"," 'последнее': 247,\n"," 'походила': 170,\n"," 'похожем': 238,\n"," 'почти': 262,\n"," 'при': 31,\n"," 'прислугой,': 179,\n"," 'приставания': 97,\n"," 'приходилась': 176,\n"," 'про': 67,\n"," 'проскользнуть': 85,\n"," 'против': 75,\n"," 'проходить': 192,\n"," 'проходя': 270,\n"," 'пятиэтажного': 172,\n"," 'раз': 268,\n"," 'раз,': 186,\n"," 'раздражительном': 241,\n"," 'с': 7,\n"," 'самому': 91,\n"," 'самою': 175,\n"," 'своей': 199,\n"," 'своею': 222,\n"," 'своими': 253,\n"," 'себя': 234,\n"," 'слушать': 70,\n"," 'совсем': 34,\n"," 'состоянии,': 239,\n"," 'стесненное': 244,\n"," 'стыдился': 277,\n"," 'сущности,': 275,\n"," 'та': 78,\n"," 'так': 287,\n"," 'то': 285,\n"," 'того': 236,\n"," 'только': 229,\n"," 'труслив': 288,\n"," 'трусливое': 259,\n"," 'тяготить': 249,\n"," 'у': 161,\n"," 'углубился': 226,\n"," 'угрозы,': 94,\n"," 'уединился': 233,\n"," 'уж,': 87,\n"," 'улизнуть,': 90,\n"," 'улицу': 211,\n"," 'улицу,': 188,\n"," 'хозяйка': 163,\n"," 'хозяйке': 281,\n"," 'хозяйки,': 258,\n"," 'хозяйкиной': 260,\n"," 'хозяйкой': 223,\n"," 'хозяйкой.': 228,\n"," 'хотел': 255,\n"," 'человек': 193,\n"," 'человек,': 269,\n"," 'чем': 167,\n"," 'чрезвычайно': 207,\n"," 'что': 47,\n"," 'чтоб': 286,\n"," 'чтобы': 56,\n"," 'чувствовал': 272,\n"," 'шкаф,': 168,\n"," 'эти': 98,\n"," 'этом': 92,\n"," 'эту': 33}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"il0jKMxH23uh"},"source":["# Заключение\n","Сегодня существует множество моделей для векторных представлений слов на разных языках. По похожему принципу строятся и векторные представления предложений и даже текстов целиком. Мы еще познакомимся с некоторыми примерами в этой области."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2021-01-16T17:12:03.387431Z","start_time":"2021-01-16T17:12:03.381430Z"},"id":"fHQmJaAJ23ui"},"source":["## Ссылки\n","\n","Использованы и адаптированы материалы:\n","\n","https://radimrehurek.com/gensim/index.html\n","\n","\n","https://medium.com/sciforce/word-vectors-in-natural-language-processing-global-vectors-glove-51339db89639\n","\n","\n","https://colab.research.google.com/github/mdda/deep-learning-workshop/blob/master/notebooks/5-RNN/3-Text-Corpus-and-Embeddings.ipynb#scrollTo=h-CQETk6HPmx\n","\n","\n","https://radimrehurek.com/gensim/models/fasttext.html \n","\n","https://amitness.com/2020/06/fasttext-embeddings/\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"fZxCwO4BPjLa"},"source":[""],"execution_count":null,"outputs":[]}]}