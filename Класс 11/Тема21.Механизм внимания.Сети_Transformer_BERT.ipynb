{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Тема21.Механизм внимания.Сети_Transformer_BERT.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3e86c3fb396b4e809921e07b04421d59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4520b6ef2762474a872453f956384cf7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ab7bdcb793184ce2baf5bbf65ad95d0a","IPY_MODEL_a9458b82008d4c0195ed0db1fab191e2"]}},"4520b6ef2762474a872453f956384cf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab7bdcb793184ce2baf5bbf65ad95d0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ad0c5223727e4a06988494e953660c75","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f62ba1ba10040b583105a15e8e86df6"}},"a9458b82008d4c0195ed0db1fab191e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_472e4003134843fd80152872710855c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 270kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea2d458725634cd58ffcc5c994631beb"}},"ad0c5223727e4a06988494e953660c75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7f62ba1ba10040b583105a15e8e86df6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"472e4003134843fd80152872710855c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ea2d458725634cd58ffcc5c994631beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"98bc637988ce43a48aaa734ad6898225":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_11252a64d403494e8e5e7110aab73ab4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0ae8fa2044934d728c97ad5f29c06468","IPY_MODEL_f05b3f85b19b4e95b4677ded232ed7a7"]}},"11252a64d403494e8e5e7110aab73ab4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ae8fa2044934d728c97ad5f29c06468":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c76493986e0b4ac1b5a1b3aad1655e2b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af9387b2774c4118bc0f1c2e52ba5aaa"}},"f05b3f85b19b4e95b4677ded232ed7a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f7da49121c9b44af98135057691b8f12","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 77.6B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d368290b46f48dc8644d29506640242"}},"c76493986e0b4ac1b5a1b3aad1655e2b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af9387b2774c4118bc0f1c2e52ba5aaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7da49121c9b44af98135057691b8f12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7d368290b46f48dc8644d29506640242":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2881b7c014f43a6a43f167d0f8eaefd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3a0d1af0ded44ee687d7d8a7df3caec0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a8cd87d76a3047faab93a55132b2b878","IPY_MODEL_6b097b68c8824dfaa08ba38f5f37d4b2"]}},"3a0d1af0ded44ee687d7d8a7df3caec0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a8cd87d76a3047faab93a55132b2b878":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ce859e0c2c0049d8b705aa898f918bd9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32d00cb7ff2d47a9a42d932904a84147"}},"6b097b68c8824dfaa08ba38f5f37d4b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a6361ab66e24977bda63d4cf1882900","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 4.81MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb0d5cd43f664f0f9c0ea7681fee6a1d"}},"ce859e0c2c0049d8b705aa898f918bd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"32d00cb7ff2d47a9a42d932904a84147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a6361ab66e24977bda63d4cf1882900":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb0d5cd43f664f0f9c0ea7681fee6a1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c90ffb93ac1d4c5ba2306cbe78d3d5f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_02df8a6f3777496ba3cb570bdefccf62","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ee948ba16635416d95303ce08fe4006d","IPY_MODEL_3b72c6c223a549ffa69eaa700fbd63b7"]}},"02df8a6f3777496ba3cb570bdefccf62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee948ba16635416d95303ce08fe4006d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3475f805aa354e5d8ca3189babfa730f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_294a7267f30f4ce4abd61fab717feae3"}},"3b72c6c223a549ffa69eaa700fbd63b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_734cc491d49348b7a25dc6023125838d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 1.24kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5fa538ea3c224877bcea420614c1d8ad"}},"3475f805aa354e5d8ca3189babfa730f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"294a7267f30f4ce4abd61fab717feae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"734cc491d49348b7a25dc6023125838d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5fa538ea3c224877bcea420614c1d8ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b68e71e99254ab98b835261acf26917":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6dae576803dd421695843756f409b0df","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b73a009f06a049b6b37cf38f3ba46874","IPY_MODEL_2d9a8a97789f4f20a6e37a6dc88a252d"]}},"6dae576803dd421695843756f409b0df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b73a009f06a049b6b37cf38f3ba46874":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1e46d6ef63154a3fb98a9727a68c2d51","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8db47b7c5974631bb88bbce324dfb4e"}},"2d9a8a97789f4f20a6e37a6dc88a252d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_863b616d0eaf4474b515ab89bb654ca8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [06:42&lt;00:00, 665kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_781a6b2e992f49b890a0febac2231776"}},"1e46d6ef63154a3fb98a9727a68c2d51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8db47b7c5974631bb88bbce324dfb4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"863b616d0eaf4474b515ab89bb654ca8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"781a6b2e992f49b890a0febac2231776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"lnjHLeHCqn2j"},"source":["#Тема 21. Механизм внимания. Сети Transformer, BERT\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x5l5hs8pl2xU"},"source":["Имея векторные представления для слов можем проводить дальнейшую обработку с этими векторами. Представив текст как последовательность слов логично использовать рекуррентные нейронные сети для такой обработки. Например, с помощью LSTM сетей можем пытаться предсказать следующее слово по уже имеющимся в предложении. \n","\n","Но для задач обработки именно текстов возникают проблемы.\n","\n","В сетях типа LSTM влияние разных элементов последовательности друг на друга устанавливается косвенно, через влияние на соседние элементы. К тому же такое влияние идет только в одну строну, предыдущие элементы влияют на последующие через изменение состояния нейрона, но не наоборот. \n","\n","Частично это исправляется в *двунаправленных сетях*, где последовательность читается и слева направо и справа налево, а результаты объединяются. \n","\n","![img](https://www.tutorialexample.com/wp-content/uploads/2020/07/The-structure-of-BiLSTM.png)\n","\n","Но даже этого оказывается мало для качественной обработки текстов. В текстах разные слова, даже находящиеся далеко от текущего, могут сильно влиять на него. \n","\n","Нужно разрешить элементам последовательности влиять друг на друга произвольно и обучать такие зависимости. \n","\n","Так работает **\"механизм внимания\"** (attention mechanism). Дальше мы будем подразумевать последовательности слов, но вообще, все изложенное применимо к любым последовательностям, например, последовательностям звуков - речи.\n","\n","# Механизм внимания\n","Давайте каждому элементу входной последовательности (это вектор: или векторные представления входной информации или выход предыдущего слоя) сопоставим три вектора: Ключ (Key) К, Запрос (Query) Q, Значение (Value) V. Их можно получить умножением вектора на некоторую матрицу, размер которой мы выбираем сами. Такие матрицы будут обучаться.\n","\n","![img](https://jalammar.github.io/images/t/self-attention-matrix-calculation.png) \n"]},{"cell_type":"markdown","metadata":{"id":"mOJd8bSYvYwT"},"source":["Для Запроса Q конкретного элемента (на картинке первого) посчитаем его скалярные произведения  с *каждым* Ключом K всех других элементов (включая себя), получим набор чисел score, поместим их в вектор и с помощью softmax переведем его в вид **коэффициентов внимания** (в диапазоне от 0 до 1). \n","\n","Такие коэффициенты показывают, насколько похожи (т.е. насколько связаны между собой) этот Запрос Q на все Ключи K. \n","\n","![img](https://miro.medium.com/max/3789/1*jf__2D8RNCzefwS0TP1Kyg.gif)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sT3LijvaphOJ"},"source":["Далее каждое Значение V умножим на соответствующий коэффициент внимания score и просуммируем результаты для всех Значений. \n","\n","Так мы создадим новый вектор output для текущего Запроса Q. Например, если все другие слова (вектора Значений V) в последовательности непохожи на слово (вектор) Запроса Q, их коэффициент внимания будет 0, и на выходе останется только Значение V от того же Запроса Q.  \n","\n","![img](https://miro.medium.com/max/3789/1*1je5TwhVAwwnIeDFvww3ew.gif)\n"]},{"cell_type":"markdown","metadata":{"id":"AMSBtdnzrBXN"},"source":["\n","Сделаем так для всех Запросов Q от разных элементов входной поледовательности.\n","\n","В результате получим выходную последовательность, в каждом элементе которой явно учтены влияния всех других элементов входной последовательности.\n","\n","\n","![img](https://miro.medium.com/max/3156/1*_92bnsMJy8Bl539G4v93yg.gif)\n","\n","\n","\n","Все описанные операции выполняются в специальном слое **внимания**, который реализован как слой нейронной сети. Принимает последовательность векторов,  возвращает другую последовательность векторов (возможно другой размерности), в которой учтено влияние входов друг на друга.   \n","\n","Матрицы Запроса, Ключа, Значения будут обучаться, поэтому компьютер сможет сам подобрать их значения. Здесь все операции дифференцируемые, поэтому обучение ведется известным нам методом обратного распространения ошибки. \n"]},{"cell_type":"markdown","metadata":{"id":"EwqkK8OP00yF"},"source":["## Множественное внимание\n","Для еще лучшей работы используют множественное внимание, когда одновременно применяют несколько разных матриц Q,K,V, а результаты потом объединяют другой обучаемой матрицей.\n","\n","![img](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)"]},{"cell_type":"markdown","metadata":{"id":"RkI4ON291b3L"},"source":["# Сеть Transformer\n","На основе механизма внимания работает сеть Transformer.\n","\n","![img](https://miro.medium.com/max/3349/1*BHzGVskWGS_3jEcYYi6miQ.png)\n","\n","Это довольно сложная нейронная сеть, состоит из двух частей, кодера и декодера.\n","\n","**Кодер** состоит из нескольких блоков. Первый блок  принимает входную последовательность, остальные блоки - выходы предыдущих блоков. Каждый блок применяет механизм множественного внимания, а также простую полносвязную сеть. Внутри блока используют перекрестные связи, по аналогии с ResNet блоками, и нормализацию.\n","\n","**Декодер** также состоит из блоков и похож на кодер, но (его первый блок) принимает *выходную* последовательность, а также выходы из соответствующего блока кодера. \n","\n","Выходы декодера обрабатываются линейным слоем и пропускаются через softmax, который возвращает уровень уверенности в следующем слове (токене) выходной последовательности.\n","\n","Поскольку это не рекуррентная сеть, то необходим дополнительный способ для кодирования времени, что выполняется в слое Positional Encoding добавлением к векторным представлениям входов вектора, отвечающего за время (какая-то последовательность, например, по функции косинуса). \n","\n","В процессе обучения входная и выходная последовательности известны. Например, для задачи перевода текста это предложения (последовательности слов) на одном и другом языке. Поэтому известны цели, чему обучать сеть - уровень уверенности должен быть максимальным для того слова, которое имеется в выходной последовательности. Сеть обучается на множестве примеров входных и выходных последовательностей.   \n","\n","В процессе предсказания выходная последовательность не известна, поэтому строится итерационно:\n","- сначала используется пустая выходная последовательность с токеном начала последовательности и предсказывается (выход softmax) следующий ее токен.\n","- этот предсказанный токен вставляется в последовательность и предсказывается следующий, и т.д., пока не будет предсказан токен конца последовательности.\n","\n","В таком подходе предсказанная выходная последовательность может быть другой длины. \n"]},{"cell_type":"markdown","metadata":{"id":"PJ1FovPS88WK"},"source":["# BERT\n","На основе сетей Transformer работает, пожалуй, одна из наиболее успешных сетей для работы с текcтом - сеть BERT (Bidirectional Encoder Representations from Transformers).\n","\n","![img](https://yashuseth.files.wordpress.com/2019/06/fig9.png?w=412&h=375)\n","\n","\n","\n","Эта сеть использует только **кодер** из Transformer который принимая входную последовательность токенов (слов) возвращает измененную последовательность (которая потом поступила бы на декодер). \n","\n","Изменили цель обучения такой сети. В обучении BERT используется два подхода:\n"]},{"cell_type":"markdown","metadata":{"id":"YHFDLqiuxLRY"},"source":["\n","## 1) Предсказание маски. Masked Language Modelling\n","Некоторое количество токенов (15%) в последовательности заменяется специальным токеном [MASK] и ставится задача правильно предсказать такие токены. Для этого кодер дополняется слоями классификации (линейный слой, активация, нормализация), проекции в пространство словаря и softmax, а в функции ошибки учитываются только выходы для токенов помеченных токеном [MASK].\n","\n","![img](https://miro.medium.com/max/3789/0*ViwaI3Vvbnd-CJSQ.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3npElCCgxOxl"},"source":["## 2) Предсказание следующей последовательности. Next Sentence Prediction\n","В качестве входа подаются *две последовательности*, одна за другой и сеть учится предсказывать, что вторая последовательность является логическим продолжением первой. \n","\n","Для этого при обучении создаются примеры как \"правильных\" последовательностей взятых подряд из обучающих данных (предложения из текста, взятые подряд), так и \"неправильных\", взятых в случайном порядке. Последовательности разделяются токеном [SEP], первая последовательность начинается с токена [CLS].\n","\n","Выход кодера (конкретно для входного [CLS])  классифицируется (добавлением одного слоя классификации с softmax) на два класса: последовательности действительно логически продолжают друг друга или нет.\n"]},{"cell_type":"markdown","metadata":{"id":"jRlGNg-Axy2N"},"source":["\n","Обучение ведется с комбинированной функцией ошибки обоих подходов.\n","\n","Чтобы можно было различить последовательности их кодируют специальным образом:\n","\n","- к векторным представлениям токенов первой последовательности добавляется специальная последовательность A, к токенам второй - B (эти специальные последовательности были предобучены)\n","- и, как и в Transformer, добавляется последовательность для кодирования времени. \n","\n","![img](https://miro.medium.com/max/3789/0*m_kXt3uqZH9e7H4w.png)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZeOMoBQUyDKN"},"source":["Обучая такую сеть на *огромном* объеме текстов получили одну из самых мощных на сегодня сетей для работы с текстом. \n","\n","Обратите внимание, что кодер Transformer переводит последовательность в другое векторное представление. Это свойство нам еще пригодится. \n","\n","Посмотрим пример. "]},{"cell_type":"markdown","metadata":{"id":"izA3-6kffbdT"},"source":["# Пример работы BERT\n","\n","Решается задача определения отзыва на фильм как положительного или отрицательного. \n","\n","Используется DistilBERT (упрощенная версия BERT) для перевода отзыва в векторное пространство (размерностью 768), а затем  логистическая регрессия для классификации.\n","\n","Сети, основанные на transformer реализованы в библиотеке [transformers](https://huggingface.co/transformers/).\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n","\n","## Набор данных\n","Используем набор данных [SST2](https://nlp.stanford.edu/sentiment/index.html), пример ниже (1 - положительные, 0 - отрицательные).\n","\n","<table class=\"features-table\">\n","  <tr>\n","    <th class=\"mdc-text-light-green-600\">\n","    sentence\n","    </th>\n","    <th class=\"mdc-text-purple-600\">\n","    label\n","    </th>\n","  </tr>\n","  <tr>\n","    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n","      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n","    </td>\n","    <td class=\"mdc-bg-purple-50\">\n","      1\n","    </td>\n","  </tr>\n","  <tr>\n","    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n","      apparently reassembled from the cutting room floor of any given daytime soap\n","    </td>\n","    <td class=\"mdc-bg-purple-50\">\n","      0\n","    </td>\n","  </tr>\n","  <tr>\n","    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n","      they presume their audience won't sit still for a sociology lesson\n","    </td>\n","    <td class=\"mdc-bg-purple-50\">\n","      0\n","    </td>\n","  </tr>\n","  <tr>\n","    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n","      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n","    </td>\n","    <td class=\"mdc-bg-purple-50\">\n","      1\n","    </td>\n","  </tr>\n","  <tr>\n","    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n","      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n","    </td>\n","    <td class=\"mdc-bg-purple-50\">\n","      1\n","    </td>\n","  </tr>\n","</table>\n","\n"]},{"cell_type":"code","metadata":{"id":"To9ENLU90WGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407181639,"user_tz":-180,"elapsed":7849,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"a0023c5b-7e60-486a-b2d3-df00850ebb9e"},"source":["# устанавливаем библиотеку \n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 24.5MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 45.0MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fvFvBLJV0Dkv","executionInfo":{"status":"ok","timestamp":1625407283669,"user_tz":-180,"elapsed":5290,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["# подключаем библиотеки\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import transformers as ppb\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ-42fh0hjsF"},"source":["Загружаем набор данных\n"]},{"cell_type":"code","metadata":{"id":"cyoj29J24hPX","executionInfo":{"status":"ok","timestamp":1625407291174,"user_tz":-180,"elapsed":658,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMVE3waNhuNj"},"source":["Оставим только 2,000 штук для скорости."]},{"cell_type":"code","metadata":{"id":"gTM3hOHW4hUY","executionInfo":{"status":"ok","timestamp":1625407295490,"user_tz":-180,"elapsed":234,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["batch_1 = df[:2000]"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PRc2L89hh1Tf"},"source":["Сколько положительных и отрицательных?"]},{"cell_type":"code","metadata":{"id":"jGvcfcCP5xpZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407297823,"user_tz":-180,"elapsed":242,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"22503e52-41b6-4d88-97ee-e13298efa4bb"},"source":["batch_1[1].value_counts()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    1041\n","0     959\n","Name: 1, dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"7_MO08_KiAOb"},"source":["### Загружаем обученную модель BERT\n"]},{"cell_type":"code","metadata":{"id":"q1InADgf5xm2","colab":{"base_uri":"https://localhost:8080/","height":348,"referenced_widgets":["3e86c3fb396b4e809921e07b04421d59","4520b6ef2762474a872453f956384cf7","ab7bdcb793184ce2baf5bbf65ad95d0a","a9458b82008d4c0195ed0db1fab191e2","ad0c5223727e4a06988494e953660c75","7f62ba1ba10040b583105a15e8e86df6","472e4003134843fd80152872710855c1","ea2d458725634cd58ffcc5c994631beb","98bc637988ce43a48aaa734ad6898225","11252a64d403494e8e5e7110aab73ab4","0ae8fa2044934d728c97ad5f29c06468","f05b3f85b19b4e95b4677ded232ed7a7","c76493986e0b4ac1b5a1b3aad1655e2b","af9387b2774c4118bc0f1c2e52ba5aaa","f7da49121c9b44af98135057691b8f12","7d368290b46f48dc8644d29506640242","e2881b7c014f43a6a43f167d0f8eaefd","3a0d1af0ded44ee687d7d8a7df3caec0","a8cd87d76a3047faab93a55132b2b878","6b097b68c8824dfaa08ba38f5f37d4b2","ce859e0c2c0049d8b705aa898f918bd9","32d00cb7ff2d47a9a42d932904a84147","0a6361ab66e24977bda63d4cf1882900","eb0d5cd43f664f0f9c0ea7681fee6a1d","c90ffb93ac1d4c5ba2306cbe78d3d5f2","02df8a6f3777496ba3cb570bdefccf62","ee948ba16635416d95303ce08fe4006d","3b72c6c223a549ffa69eaa700fbd63b7","3475f805aa354e5d8ca3189babfa730f","294a7267f30f4ce4abd61fab717feae3","734cc491d49348b7a25dc6023125838d","5fa538ea3c224877bcea420614c1d8ad","1b68e71e99254ab98b835261acf26917","6dae576803dd421695843756f409b0df","b73a009f06a049b6b37cf38f3ba46874","2d9a8a97789f4f20a6e37a6dc88a252d","1e46d6ef63154a3fb98a9727a68c2d51","e8db47b7c5974631bb88bbce324dfb4e","863b616d0eaf4474b515ab89bb654ca8","781a6b2e992f49b890a0febac2231776"]},"executionInfo":{"status":"ok","timestamp":1625407356663,"user_tz":-180,"elapsed":8321,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"c4490a4e-da33-4292-93e5-e72140a07472"},"source":["#  DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Хотите BERT вместо distilBERT? Раскоментируйте\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# загружаем\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights) # токенизатор для разделения на слова\n","model = model_class.from_pretrained(pretrained_weights) # модель "],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e86c3fb396b4e809921e07b04421d59","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98bc637988ce43a48aaa734ad6898225","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2881b7c014f43a6a43f167d0f8eaefd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c90ffb93ac1d4c5ba2306cbe78d3d5f2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b68e71e99254ab98b835261acf26917","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lZDBMn3wiSX6"},"source":["Надо сделать некоторую предобработку данных.\n","\n","### Токенизация\n","Разбиваем предложения на токены в подходящем формате:\n","- разбиваем отзыв на отдельные токены (слова, части слов и т.п.)\n","- добавляем токены CLS и SEP\n","- заменяем токены на их номер ID\n","\n"]},{"cell_type":"code","metadata":{"id":"Dg82ndBA5xlN","executionInfo":{"status":"ok","timestamp":1625407439010,"user_tz":-180,"elapsed":1405,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHwjUwYgi-uL"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n","\n","### Набивка\n","Наиболее быстрая работа когда все предложения в одном массиве, для чего их надо дополнить до одинакового размера."]},{"cell_type":"code","metadata":{"id":"URn-DWJt5xhP","executionInfo":{"status":"ok","timestamp":1625407689336,"user_tz":-180,"elapsed":478,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["max_len = 0\n","for i in tokenized.values:\n","    if len(i) > max_len:\n","        max_len = len(i)\n","\n","padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdi7uXo95xeq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407692523,"user_tz":-180,"elapsed":346,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"c5125511-dee0-4efc-f9b5-7ec78ba41b58"},"source":["np.array(padded).shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000, 59)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"sDZBsYSDjzDV"},"source":["Надо указать, что добавленные в набивке нулевые значения нужно игнорировать. "]},{"cell_type":"code","metadata":{"id":"4K_iGRNa_Ozc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407701570,"user_tz":-180,"elapsed":260,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"f068414e-93a1-430e-f671-1bae38b49f03"},"source":["attention_mask = np.where(padded != 0, 1, 0)\n","attention_mask.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000, 59)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"jK-CQB9-kN99"},"source":["## Запускаем модель\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n","\n","Команда`model()` запускает расчет модели BERT, результат вернется в `last_hidden_states`."]},{"cell_type":"code","metadata":{"id":"39UVjAV56PJz","executionInfo":{"status":"ok","timestamp":1625407843069,"user_tz":-180,"elapsed":90351,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["input_ids = torch.tensor(padded)  # тензоры\n","attention_mask = torch.tensor(attention_mask) # тензоры\n","\n","with torch.no_grad():\n","    last_hidden_states = model(input_ids, attention_mask=attention_mask) # запускаем"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FoCep_WVuB3v"},"source":["При классификации добавляется токен `[CLS]` в начале каждого предложения, выход для него и есть векторное представление для всего предложения.  \n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n","\n"]},{"cell_type":"code","metadata":{"id":"C9t60At16PVs","executionInfo":{"status":"ok","timestamp":1625407879599,"user_tz":-180,"elapsed":240,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["features = last_hidden_states[0][:,0,:].numpy() # это векторное представление отзывов"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VZVU66Gurr-"},"source":["Метки классов:"]},{"cell_type":"code","metadata":{"id":"JD3fX2yh6PTx","executionInfo":{"status":"ok","timestamp":1625407882242,"user_tz":-180,"elapsed":235,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["labels = batch_1[1] # метки классов"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddAqbkoU6PP9","executionInfo":{"status":"ok","timestamp":1625407887187,"user_tz":-180,"elapsed":233,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}}},"source":["# разделяем данные на обучающие и тестовые\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9bhSJpcv1Bl"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n"]},{"cell_type":"markdown","metadata":{"id":"KCT9u8vAwnID"},"source":["### Классификация\n","После применения BERT имеем некоторые вектора для каждого отзыва. Создаем и обучаем логистическую регресию."]},{"cell_type":"code","metadata":{"id":"gG-EVWx4CzBc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407936644,"user_tz":-180,"elapsed":249,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"f4571b64-5c55-44a0-c5b9-50d374866090"},"source":["lr_clf = LogisticRegression() # создаем\n","lr_clf.fit(train_features, train_labels) # обучаем"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"iCoyxRJ7ECTA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625407939482,"user_tz":-180,"elapsed":268,"user":{"displayName":"ИТ-класс в московской школе","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS_ha7QOa0FMhe4Cp-P78YhxGwsJVkSvH-MURQ=s64","userId":"09176086438558815619"}},"outputId":"78a0c3bf-7421-4dbf-9471-007b4a96e929"},"source":["# проверяем, как работает\n","lr_clf.score(test_features, test_labels)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.866"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"7Lg4LOpoxSOR"},"source":["Получили 86% аккуратности предсказания на тестовых примерах. Это и не самая плохая и не самая хорошая классификация. \n","\n","Здесь можно посмотреть лучшие из [достигнутых результатов](http://nlpprogress.com/english/sentiment_analysis.html) для этого набора данных (**96.8**).\n"]},{"cell_type":"markdown","metadata":{"id":"wwetYL7p33y2"},"source":["#Задания\n","Попробуйте самостоятельно сделать классификацию написанного вами отзыва на фильм (на англ. языке) используя обученные здесь модели.\n","\n","(нужно повторить процесс: токенизация, набивка, расчет BERT и получение вектора для отзыва, его классификация)"]},{"cell_type":"markdown","metadata":{"id":"qy-4bugcqbq4"},"source":["# Ссылки\n","\n","Использованы и адаптированы материалы:\n","\n","Механизм внимания:\n","https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#8481\n","\n","Это самое лучшее объяснение работы Transformer:\n","https://jalammar.github.io/illustrated-transformer/\n","\n","https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04 \n","\n","BERT \n","\n","https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n","\n","http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n","\n","https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/\n"]}]}